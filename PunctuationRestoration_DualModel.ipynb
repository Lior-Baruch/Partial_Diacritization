{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "from torch.nn import Transformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import random\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "### here we need to load the data and extract only data with vowels punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the jason file for the dataset\n",
    "data_json_path = 'data/books.json'\n",
    "\n",
    "# Root directory where the downloaded files will be saved\n",
    "texts_path = 'data/texts'\n",
    "\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(texts_path):\n",
    "    os.makedirs(texts_path)\n",
    "\n",
    "\n",
    "# Load the json dataset\n",
    "with open(data_json_path, 'r', encoding='utf-8') as f:\n",
    "    jason_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# download the files and save them in a folder\n",
    "\n",
    "#### remove\\add the comment as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loop through the json dataset and download the files\n",
    "# for entry in tqdm(jason_data):\n",
    "#     try:\n",
    "#         # Download the Nikud Meteg file\n",
    "#         if entry['fileName'] + '__nikud_meteg' in os.listdir(texts_path):\n",
    "#             continue\n",
    "#         nikud_meteg_url = entry['nikudMetegFileURL']\n",
    "#         nikud_meteg_local_path = os.path.join(texts_path, entry['fileName'] + '__nikud_meteg.zip')\n",
    "#         nikud_meteg_response = requests.get(nikud_meteg_url)\n",
    "#         with open(nikud_meteg_local_path, 'wb') as f:\n",
    "#             f.write(nikud_meteg_response.content)\n",
    "\n",
    "#             # Unzip the Nikud Meteg file\n",
    "#             with ZipFile(nikud_meteg_local_path, 'r') as zipObj:\n",
    "#                 zipObj.extractall(os.path.join(texts_path, entry['fileName'] + '__nikud_meteg'))\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading file {entry['fileName']}: {e}\")\n",
    "#         continue\n",
    "\n",
    "\n",
    "# # iterate through the texts folder and delete the zip folders\n",
    "# for file in tqdm(os.listdir(texts_path)):\n",
    "#     if file.endswith(\".zip\"):\n",
    "#         os.remove(os.path.join(texts_path, file))\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author files\n",
    "\n",
    "### Create a dictionary whose keys are authors and values are a list containing all it's files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a method to create the author files dictionary\n",
    "def create_author_files_dict(author_files):\n",
    "    \"\"\"\n",
    "    This function creates a dictionary of author files with a list of their corresponding texts.\n",
    "    \"\"\"\n",
    "    author_files_dict = {}\n",
    "    for file in author_files:\n",
    "        author_files_dict[file] = []\n",
    "        for text_file_name in os.listdir(os.path.join(texts_path, file)):\n",
    "            if text_file_name.endswith('.txt'):\n",
    "                author_files_dict[file].append(text_file_name)\n",
    "    return author_files_dict\n",
    "\n",
    "author_files = os.listdir(texts_path)\n",
    "author_files_dict = create_author_files_dict(author_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nikud unicode range (https://en.wikipedia.org/wiki/Unicode_and_HTML_for_the_Hebrew_alphabet)\n",
    "\n",
    "# Read a txt file from the author files dictionary\n",
    "def read_txt_file(file_path):\n",
    "    \"\"\"\n",
    "    This function reads a txt file and returns the text as a string.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def remove_nikud(string):\n",
    "    \"\"\"Removes the nikud from the given string.\"\"\"\n",
    "    nikud = re.compile(r'[\\u05B0-\\u05C2]')\n",
    "    return nikud.sub(\"\", string)\n",
    "\n",
    "def get_nikud(word):\n",
    "    \"\"\"Returns the nikud from the given word.\"\"\"\n",
    "    nikud = re.compile(r'[\\u05B0-\\u05C2]')\n",
    "    current_nikud = ''\n",
    "    nikud_arr = []\n",
    "    for i in range(len(word)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if nikud.match(word[i]):\n",
    "            current_nikud += word[i]\n",
    "        else:\n",
    "            nikud_arr.append(current_nikud)\n",
    "            current_nikud = ''\n",
    "    nikud_arr.append(current_nikud)\n",
    "    return nikud_arr\n",
    "\n",
    "def add_nikud(word, nikud):\n",
    "    \"\"\"Adds the nikud to the given word.\"\"\"\n",
    "    new_word = ''\n",
    "    for i in range(len(word)):\n",
    "        new_word += word[i] + nikud[i]\n",
    "    return new_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data to sentences and save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_txt_file(os.path.join(texts_path, 'afikeiyam1__nikud_meteg', 'afikeiyam1-002__nikud_meteg.txt'))\n",
    "text_after_split = re.split(r'\\n|\\.', text)\n",
    "print(text_after_split)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a unified csv of all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "columns = ['text_with_nikud', 'text_without_nikud', 'nikud', 'author', 'file_name', 'sentence_num']\n",
    "# with open('data/full_data.csv', 'w', encoding='utf-8') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow(columns)\n",
    "#     for author in tqdm(author_files_dict):\n",
    "#         for file in author_files_dict[author]:\n",
    "#             text = read_txt_file(os.path.join(texts_path, author, file))\n",
    "#             # split the text into sentences by \\n or .\n",
    "#             sentences = re.split(r'\\n|\\.', text)\n",
    "#             for i, sentence in enumerate(sentences):\n",
    "#                 sentence_words = sentence.split()\n",
    "#                 # remove the nikud from the sentence\n",
    "#                 sentence_without_nikud = remove_nikud(sentence)\n",
    "#                 # get the nikud from the sentence\n",
    "#                 nikud = list(map(get_nikud, sentence_words))\n",
    "\n",
    "#                 # add the sentence to the dataframe\n",
    "#                 writer.writerow([sentence, sentence_without_nikud, nikud, author, file, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     text_with_nikud   \n",
      "0  פֶּתַח דָּבָר  יִתְבָּרֵךְ הַבּוֹרֵא וְיִשְׁתּ...  \\\n",
      "1                     וְלֵיֽהָנוֹת מִזִּיו הַתּוֹרָה   \n",
      "2   וַאֲשֶׁר לֹא עָזַב חַסְדּוֹ זֶה מִמֶּנִּי עַד...   \n",
      "3   וְהִיא שֶׁעָמְדָה לִי לְחַדֵּשׁ חִיֽדּוּשִׁים...   \n",
      "4                          כְּיָד ה' הַטּוֹבָה עָלַי   \n",
      "\n",
      "                                  text_without_nikud   \n",
      "0  פתח דבר  יתברך הבורא וישתבח היוצר אשר מעודי גב...  \\\n",
      "1                                 וליהנות מזיו התורה   \n",
      "2                   ואשר לא עזב חסדו זה ממני עד היום   \n",
      "3   והיא שעמדה לי לחדש חידושים בענינים שונים בש\"ס...   \n",
      "4                                   כיד ה' הטובה עלי   \n",
      "\n",
      "                                               nikud                   author   \n",
      "0  [[ֶּ, ַ, ], [ָּ, ָ, ], [ִ, ְ, ָּ, ֵ, ְ], [ַ, ּ...  afikeiyam1__nikud_meteg  \\\n",
      "1  [[ְ, ֵ, ֽ, ָ, , ֹ, ], [ִ, ִּ, , ], [ַ, ּ, ֹ, ָ...  afikeiyam1__nikud_meteg   \n",
      "2  [[ַ, ֲ, ֶׁ, ], [ֹ, ], [ָ, ַ, ], [ַ, ְ, ּ, ֹ], ...  afikeiyam1__nikud_meteg   \n",
      "3  [[ְ, ִ, , ], [ֶׁ, ָ, ְ, ָ, ], [ִ, ], [ְ, ַ, ֵּ...  afikeiyam1__nikud_meteg   \n",
      "4        [[ְּ, ָ, ], [, ], [ַ, ּ, ֹ, ָ, ], [ָ, ַ, ]]  afikeiyam1__nikud_meteg   \n",
      "\n",
      "                         file_name  sentence_num  \n",
      "0  afikeiyam1-002__nikud_meteg.txt             0  \n",
      "1  afikeiyam1-002__nikud_meteg.txt             1  \n",
      "2  afikeiyam1-002__nikud_meteg.txt             2  \n",
      "3  afikeiyam1-002__nikud_meteg.txt             3  \n",
      "4  afikeiyam1-002__nikud_meteg.txt             4  \n"
     ]
    }
   ],
   "source": [
    "# dataframe of the CSV with chunksize of 1000\n",
    "# data_df_chunks = pd.read_csv('data/full_data.csv', chunksize=1000)\n",
    "\n",
    "# # read only first 100000 rows of the CSV, we will use this for now\n",
    "# data_df = pd.read_csv('data/full_data.csv', nrows=100000, converters={'nikud': eval})\n",
    "# print(data_df.head())\n",
    "\n",
    "# # keep only rows s.t the length of the text (without spaces) is at most 100\n",
    "# data_df = data_df[data_df['text_without_nikud'].str.replace(' ', '').str.len() <= 100]\n",
    "\n",
    "# # save the dataframe to a json file\n",
    "# data_df.to_json('data/short_data_df.json', orient='records', lines=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57492, 6)\n"
     ]
    }
   ],
   "source": [
    "# read the json file\n",
    "data_df = pd.read_json('data/short_data_df.json', orient='records', lines=True)\n",
    "\n",
    "print(data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57492/57492 [00:00<00:00, 110615.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ֶּ': 0, 'ַ': 1, '<no_nikud>': 2, 'ָּ': 3, 'ָ': 4, 'ִ': 5, 'ְ': 6, 'ֵ': 7, 'ּ': 8, 'ֹ': 9, 'ְׁ': 10, 'ַּ': 11, 'ֲ': 12, 'ֶׁ': 13, 'ׂ': 14, 'ֶ': 15, 'ֵּ': 16, 'ֵׁ': 17, 'ְּ': 18, 'ֽ': 19, 'ִּ': 20, 'ׁ': 21, 'ִׁ': 22, 'ַׁ': 23, 'ָׂ': 24, 'ָׁ': 25, 'ֱ': 26, 'ְׂ': 27, 'ֵּׂ': 28, 'ָּׂ': 29, 'ּׁ': 30, 'ֳ': 31, 'ֻ': 32, 'ֹּ': 33, 'ִׂ': 34, 'ֵׂ': 35, 'ַּׁ': 36, 'ֵּׁ': 37, 'ְּׁ': 38, 'ֻּ': 39, 'ִּׁ': 40, 'ֶׂ': 41, 'ָּׁ': 42, 'ֻׁ': 43, 'ֹׁ': 44, 'ֶּׁ': 45, 'ַּׂ': 46, 'ּׂ': 47, 'ֳּ': 48, 'ַׂ': 49, 'ְּׂ': 50, 'ִּׂ': 51, 'ֻּׁ': 52, 'ֶּׂ': 53, 'ַָ': 54, 'ֲּ': 55, 'ֹּׁ': 56, 'ַָ': 57, 'ֹּׂ': 58, 'ֹׂ': 59, 'ִַ': 60, 'ֲׁ': 61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "label_to_id = {}\n",
    "id_to_label = {}\n",
    "for label_list in tqdm(data_df['nikud']):\n",
    "    # flatten the list\n",
    "    label_list = [item for sublist in label_list for item in sublist]\n",
    "    for label in label_list:\n",
    "        if len(label) == 0:\n",
    "            label = \"<no_nikud>\"\n",
    "        if label not in label_to_id:\n",
    "            label_to_id[label] = len(label_to_id)\n",
    "            id_to_label[len(id_to_label)] = label\n",
    "\n",
    "print(label_to_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download tokenizer and model\n",
    "(alephbert-base, with vocab of words with len <= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'C:\\\\Users\\\\baruc\\\\PycharmProjects\\\\pythonProject\\\\Punctuation_Restoration\\\\AlephBERT-main\\\\AlephBERT-main\\\\models\\\\alephbert-base'\n",
    "alephbert_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "alephbert_model = AutoModelForMaskedLM.from_pretrained(\"onlplab/alephbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "בדיקה של הדבר הזה\n",
      "tokenized:  ['ב', '##ד', '##י', '##ק', '##ה', 'ש', '##ל', 'ה', '##ד', '##ב', '##ר', 'ה', '##ז', '##ה']\n",
      "encoded:  [2, 177, 1039, 1008, 1013, 1016, 201, 1009, 180, 1039, 1037, 1014, 180, 1075, 1016, 3]\n",
      "decoded:  [CLS] בדיקה של הדבר הזה [SEP]\n"
     ]
    }
   ],
   "source": [
    "# test the tokenization and detokenization\n",
    "test = \"בדיקה של הדבר הזה\"\n",
    "tokenized = alephbert_tokenizer.tokenize(test)\n",
    "encoded = alephbert_tokenizer.encode(test)\n",
    "decoded = alephbert_tokenizer.decode(encoded)\n",
    "print(test)\n",
    "print(\"tokenized: \", tokenized)\n",
    "print(\"encoded: \", encoded)\n",
    "print(\"decoded: \", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create DataSet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57492\n",
      "dict_keys(['text', 'nikud', 'input_ids', 'attention_mask'])\n",
      "פתחדבריתברךהבוראוישתבחהיוצראשרמעודיגברעליחסדולשוםחלקיביןחובשיביהמ\"דלהתחמםכנגדאורןשלחכמים\n",
      "88\n",
      "tensor([ 0,  1,  2,  3,  4,  2,  5,  6,  3,  7,  6,  1,  8,  9,  7,  2,  6,  5,\n",
      "        10, 11, 11,  2,  1,  8,  9,  7,  2, 12, 13,  2,  7,  2,  8,  1,  2,  3,\n",
      "         1,  2,  4,  1,  2,  1,  6,  8,  9,  4, 14,  8,  2, 15,  6,  5,  2, 16,\n",
      "         2,  2,  2,  9,  6, 17,  2,  2,  2,  2,  2,  2,  2,  6,  5,  6,  1, 16,\n",
      "         2, 18, 15, 15,  2,  2,  8,  4,  2, 13,  2, 12,  4,  5,  2,  2, 62, 62,\n",
      "        62, 62, 62, 62, 62, 62, 62, 62, 62, 62])\n",
      "torch.Size([100])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# create pytorch dataset class for punctuation restoration (returns input(text) and target(nikud))\n",
    "\n",
    "class PunctuationRestorationDataset(Dataset):\n",
    "    def __init__(self, data_df, tokenizer, label_to_id, max_len):\n",
    "        self.data = data_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label_to_id = label_to_id \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index): # TODO: need to make sure not to look at fraction of words\n",
    "        text = self.data.iloc[index]['text_without_nikud']\n",
    "        nikud = self.data.iloc[index]['nikud']  # list of lists of nikud\n",
    "\n",
    "        # flatten nikud list\n",
    "        nikud = [item for sublist in nikud for item in sublist]\n",
    "\n",
    "        # replace empty strings with <no_nikud> token\n",
    "        nikud = [label if label != \"\" else \"<no_nikud>\" for label in nikud]\n",
    "        # replace labels with ids\n",
    "        nikud = [self.label_to_id[label] for label in nikud]\n",
    "\n",
    "        # truncate if needed\n",
    "        #nikud = nikud[:self.max_len] #TODO: check if correct (should be the same as text)\n",
    "\n",
    "        # pad if needed\n",
    "        nikud = nikud + [len(label_to_id)] * (self.max_len - len(nikud))\n",
    "\n",
    "        # convert to tensor\n",
    "        nikud = torch.tensor(nikud, dtype=torch.long)\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'nikud': nikud,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "\n",
    "# create pytorch dataset class for punctuation restoration (returns input(text) and target(nikud))\n",
    "dataset = PunctuationRestorationDataset(data_df, alephbert_tokenizer, label_to_id, 100)\n",
    "sample = dataset[0]\n",
    "print(len(dataset))\n",
    "print(sample.keys())\n",
    "# remove spaces from text\n",
    "print(sample['text'].replace(\" \", \"\"))\n",
    "print(len(sample['text'].replace(\" \", \"\")))\n",
    "print((sample['nikud']))\n",
    "print(sample['input_ids'].shape)\n",
    "print(sample['attention_mask'].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset to train, val and test\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# create dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# performe EDAs on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset size:  41393\n",
      "val dataset size:  10349\n",
      "test dataset size:  5750\n"
     ]
    }
   ],
   "source": [
    "# EDA on train, val and test dataset\n",
    "print(\"train dataset size: \", len(train_dataset))\n",
    "print(\"val dataset size: \", len(val_dataset))\n",
    "print(\"test dataset size: \", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the two models (one with look-ahead, one without)\n",
    "\n",
    "### train the two models\n",
    "\n",
    "### Evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a charachter level transformer model with the following architecture:\n",
    "# 1. Embedding layer\n",
    "# 2. Positional encoding layer\n",
    "# 3. N Transformer blocks\n",
    "# 4. Linear layer to predict punctuation\n",
    "\n",
    "\n",
    "\n",
    "class FullSentenceModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=512, nhead=8, num_layers=6, output_dim=len(label_to_id)):\n",
    "        super(FullSentenceModel, self).__init__()\n",
    "        \n",
    "        # Non-pretrained embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # 6-layer transformer with 8 attention heads\n",
    "        # self.transformer = Transformer(\n",
    "        #     d_model=embedding_dim,\n",
    "        #     nhead=nhead,\n",
    "        #     num_encoder_layers=num_layers,\n",
    "        #     num_decoder_layers=num_layers\n",
    "        # )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # Pass input through embedding layer\n",
    "        characters_embeddings = self.embedding(sentence)\n",
    "        \n",
    "        # Pass through transformer\n",
    "        characters_after_transformer_layers = self.transformer_encoder(characters_embeddings)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        predictions = self.fc(characters_after_transformer_layers)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1294/1294 [01:55<00:00, 11.17it/s]\n",
      "100%|██████████| 324/324 [00:11<00:00, 28.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 4.1605, Val Loss: 4.1568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1294/1294 [01:52<00:00, 11.48it/s]\n",
      "100%|██████████| 324/324 [00:11<00:00, 29.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 4.1606, Val Loss: 4.1576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1294/1294 [01:53<00:00, 11.45it/s]\n",
      "100%|██████████| 324/324 [00:11<00:00, 29.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 4.1607, Val Loss: 4.1565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1294/1294 [01:53<00:00, 11.41it/s]\n",
      "100%|██████████| 324/324 [00:11<00:00, 29.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train Loss: 4.1608, Val Loss: 4.1572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1294/1294 [01:53<00:00, 11.41it/s]\n",
      "100%|██████████| 324/324 [00:11<00:00, 29.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train Loss: 4.1606, Val Loss: 4.1566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1294/1294 [01:52<00:00, 11.48it/s]\n",
      "100%|██████████| 324/324 [00:10<00:00, 29.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train Loss: 4.1605, Val Loss: 4.1567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1294/1294 [01:52<00:00, 11.52it/s]\n",
      "100%|██████████| 324/324 [00:11<00:00, 28.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train Loss: 4.1604, Val Loss: 4.1572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1294/1294 [01:55<00:00, 11.24it/s]\n",
      "100%|██████████| 324/324 [00:11<00:00, 29.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train Loss: 4.1605, Val Loss: 4.1569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1294/1294 [01:52<00:00, 11.49it/s]\n",
      "100%|██████████| 324/324 [00:11<00:00, 29.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train Loss: 4.1606, Val Loss: 4.1570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 156/1294 [00:13<01:38, 11.50it/s]"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=len(label_to_id)).to(device)\n",
    "\n",
    "vocab_size = len(alephbert_tokenizer.vocab)\n",
    "\n",
    "model = FullSentenceModel(vocab_size).to(device)\n",
    "\n",
    "# define the training loop\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['nikud'].to(device)\n",
    "        predictions = model.forward(input_ids)\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        labels = labels.view(-1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# define the evaluation loop\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['nikud'].to(device)\n",
    "            predictions = model.forward(input_ids)\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            labels = labels.view(-1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# define the training and evaluation loop\n",
    "def train_and_evaluate(model, train_dataloader, val_dataloader, optimizer, criterion, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train(model, train_dataloader, optimizer, criterion)\n",
    "        val_loss = evaluate(model, val_dataloader, criterion)\n",
    "        print(f'Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "# train the model\n",
    "train_and_evaluate(model, train_loader, val_loader, optimizer, criterion, epochs=10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the dual model class, it will be composed of two models.\n",
    "#### whenever there is a disagreement between the two models, the model will add nikud using the lookahead model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the dual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
