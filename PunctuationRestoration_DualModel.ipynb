{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "from torch.nn import Transformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import random\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "### here we need to load the data and extract only data with vowels punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the jason file for the dataset\n",
    "data_json_path = 'data/books.json'\n",
    "\n",
    "# Root directory where the downloaded files will be saved\n",
    "texts_path = 'data/texts'\n",
    "\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(texts_path):\n",
    "    os.makedirs(texts_path)\n",
    "\n",
    "\n",
    "# Load the json dataset\n",
    "with open(data_json_path, 'r', encoding='utf-8') as f:\n",
    "    jason_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# download the files and save them in a folder\n",
    "\n",
    "#### remove\\add the comment as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loop through the json dataset and download the files\n",
    "# for entry in tqdm(jason_data):\n",
    "#     try:\n",
    "#         # Download the Nikud Meteg file\n",
    "#         if entry['fileName'] + '__nikud_meteg' in os.listdir(texts_path):\n",
    "#             continue\n",
    "#         nikud_meteg_url = entry['nikudMetegFileURL']\n",
    "#         nikud_meteg_local_path = os.path.join(texts_path, entry['fileName'] + '__nikud_meteg.zip')\n",
    "#         nikud_meteg_response = requests.get(nikud_meteg_url)\n",
    "#         with open(nikud_meteg_local_path, 'wb') as f:\n",
    "#             f.write(nikud_meteg_response.content)\n",
    "\n",
    "#             # Unzip the Nikud Meteg file\n",
    "#             with ZipFile(nikud_meteg_local_path, 'r') as zipObj:\n",
    "#                 zipObj.extractall(os.path.join(texts_path, entry['fileName'] + '__nikud_meteg'))\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading file {entry['fileName']}: {e}\")\n",
    "#         continue\n",
    "\n",
    "\n",
    "# # iterate through the texts folder and delete the zip folders\n",
    "# for file in tqdm(os.listdir(texts_path)):\n",
    "#     if file.endswith(\".zip\"):\n",
    "#         os.remove(os.path.join(texts_path, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author files\n",
    "\n",
    "### Create a dictionary whose keys are authors and values are a list containing all it's files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a method to create the author files dictionary\n",
    "def create_author_files_dict(author_files):\n",
    "    \"\"\"\n",
    "    This function creates a dictionary of author files with a list of their corresponding texts.\n",
    "    \"\"\"\n",
    "    author_files_dict = {}\n",
    "    for file in author_files:\n",
    "        author_files_dict[file] = []\n",
    "        for text_file_name in os.listdir(os.path.join(texts_path, file)):\n",
    "            if text_file_name.endswith('.txt'):\n",
    "                author_files_dict[file].append(text_file_name)\n",
    "    return author_files_dict\n",
    "\n",
    "author_files = os.listdir(texts_path)\n",
    "author_files_dict = create_author_files_dict(author_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nikud unicode range (https://en.wikipedia.org/wiki/Unicode_and_HTML_for_the_Hebrew_alphabet)\n",
    "\n",
    "# Read a txt file from the author files dictionary\n",
    "def read_txt_file(file_path):\n",
    "    \"\"\"\n",
    "    This function reads a txt file and returns the text as a string.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def remove_nikud(string):\n",
    "    \"\"\"Removes the nikud from the given string.\"\"\"\n",
    "    nikud = re.compile(r'[\\u05B0-\\u05C2]')\n",
    "    return nikud.sub(\"\", string)\n",
    "\n",
    "def get_nikud(word):\n",
    "    \"\"\"Returns the nikud from the given word.\"\"\"\n",
    "    nikud = re.compile(r'[\\u05B0-\\u05C2]')\n",
    "    current_nikud = ''\n",
    "    nikud_arr = []\n",
    "    for i in range(len(word)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if nikud.match(word[i]):\n",
    "            current_nikud += word[i]\n",
    "        else:\n",
    "            nikud_arr.append(current_nikud)\n",
    "            current_nikud = ''\n",
    "    nikud_arr.append(current_nikud)\n",
    "    return nikud_arr\n",
    "\n",
    "def add_nikud(word, nikud):\n",
    "    \"\"\"Adds the nikud to the given word.\"\"\"\n",
    "    new_word = ''\n",
    "    for i in range(len(word)):\n",
    "        new_word += word[i] + nikud[i]\n",
    "    return new_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test nikud functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the functions for adding and removing nikud\n",
    "text = read_txt_file(os.path.join(texts_path, 'afikeiyam1__nikud_meteg', 'afikeiyam1-002__nikud_meteg.txt'))\n",
    "# take just the first 100 characters\n",
    "text = text[:100]\n",
    "text_no_nikud = remove_nikud(text)\n",
    "text_nikud = get_nikud(text)\n",
    "text_with_nikud = add_nikud(text_no_nikud, text_nikud)\n",
    "print(\"original text:\\n\", text)\n",
    "print(\"text with added nikud:\\n\", text_with_nikud)\n",
    "print(\"text without nikud:\\n\", text_no_nikud)\n",
    "print(\"nikud array:\\n\", text_nikud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a unified csv of all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "columns = ['text_with_nikud', 'text_without_nikud', 'nikud', 'author', 'file_name', 'sentence_num']\n",
    "# with open('data/full_data.csv', 'w', encoding='utf-8') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow(columns)\n",
    "#     for author in tqdm(author_files_dict):\n",
    "#         for file in author_files_dict[author]:\n",
    "#             text = read_txt_file(os.path.join(texts_path, author, file))\n",
    "#             # split the text into sentences by \\n or .\n",
    "#             sentences = re.split(r'\\n|\\.', text)\n",
    "#             for i, sentence in enumerate(sentences):\n",
    "#                 sentence_words = sentence.split()\n",
    "#                 # remove the nikud from the sentence\n",
    "#                 sentence_without_nikud = remove_nikud(sentence)\n",
    "#                 # get the nikud from the sentence\n",
    "#                 nikud = list(map(get_nikud, sentence_words))\n",
    "\n",
    "#                 # add the sentence to the dataframe\n",
    "#                 writer.writerow([sentence, sentence_without_nikud, nikud, author, file, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load the data from the csv for chuncks and save first chunck in json for short data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe of the CSV with chunksize of 1000\n",
    "# data_df_chunks = pd.read_csv('data/full_data.csv', chunksize=1000)\n",
    "\n",
    "# # read only first 100000 rows of the CSV, we will use this for now\n",
    "# data_df = pd.read_csv('data/full_data.csv', nrows=100000, converters={'nikud': eval})\n",
    "# print(data_df.head())\n",
    "\n",
    "# # keep only rows s.t the length of the text (without spaces) is at most 100\n",
    "# data_df = data_df[data_df['text_without_nikud'].str.replace(' ', '').str.len() <= 100]\n",
    "\n",
    "# # save the dataframe to a json file\n",
    "# data_df.to_json('data/short_data_df.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### read the json short data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the json file\n",
    "data_df = pd.read_json('data/short_data_df.json', orient='records', lines=True)\n",
    "\n",
    "print(data_df.shape)\n",
    "print(data_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define dictionary label_to_id and id_to_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id = {}\n",
    "id_to_label = {}\n",
    "for label_list in tqdm(data_df['nikud']):\n",
    "    # flatten the list\n",
    "    label_list = [item for sublist in label_list for item in sublist]\n",
    "    for label in label_list:\n",
    "        if len(label) == 0:\n",
    "            label = \"<no_nikud>\"\n",
    "        if label not in label_to_id:\n",
    "            label_to_id[label] = len(label_to_id)\n",
    "            id_to_label[len(id_to_label)] = label\n",
    "\n",
    "print(label_to_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count labels for label_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of each label\n",
    "label_count = {}\n",
    "for label_list in tqdm(data_df['nikud']):\n",
    "    # Flatten the list\n",
    "    label_list = [item for sublist in label_list for item in sublist]\n",
    "    for label in label_list:\n",
    "        if len(label) == 0:\n",
    "            label = \"<no_nikud>\"\n",
    "        if label not in label_count:\n",
    "            label_count[label] = 0\n",
    "        label_count[label] += 1\n",
    "\n",
    "print(label_count)\n",
    "\n",
    "# Plot counts of each label (sorted)\n",
    "sorted_labels = sorted(label_count.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_labels)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Label Counts')\n",
    "plt.bar([x[0] for x in sorted_labels], [x[1] for x in sorted_labels])\n",
    "plt.show()\n",
    "\n",
    "# Define the label weights\n",
    "label_weights = {}\n",
    "for label in label_count:\n",
    "    label_weights[label] = 1 / label_count[label]\n",
    "\n",
    "# Normalize the weights\n",
    "sum_weights = sum(label_weights.values())\n",
    "for label in label_weights:\n",
    "    label_weights[label] /= sum_weights\n",
    "\n",
    "print(label_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download tokenizer and model\n",
    "(alephbert-base, with vocab of words with len <= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'C:\\\\Users\\\\baruc\\\\PycharmProjects\\\\pythonProject\\\\Punctuation_Restoration\\\\AlephBERT-main\\\\AlephBERT-main\\\\models\\\\alephbert-base'\n",
    "alephbert_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "alephbert_model = AutoModelForMaskedLM.from_pretrained(\"onlplab/alephbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the tokenization and detokenization\n",
    "test = \"בדיקה של הדבר הזה\"\n",
    "tokenized = alephbert_tokenizer.tokenize(test)\n",
    "encoded = alephbert_tokenizer.encode(test)\n",
    "decoded = alephbert_tokenizer.decode(encoded)\n",
    "print(test)\n",
    "print(\"tokenized: \", tokenized)\n",
    "print(\"encoded: \", encoded)\n",
    "print(\"decoded: \", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create DataSet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch dataset class for punctuation restoration (returns input(text) and target(nikud))\n",
    "\n",
    "class PunctuationRestorationDataset(Dataset):\n",
    "    def __init__(self, data_df, tokenizer, label_to_id, max_len):\n",
    "        self.data = data_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label_to_id = label_to_id \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index): # TODO: need to make sure not to look at fraction of words\n",
    "        text = self.data.iloc[index]['text_without_nikud']\n",
    "        nikud = self.data.iloc[index]['nikud']  # list of lists of nikud\n",
    "\n",
    "        # flatten nikud list\n",
    "        nikud = [item for sublist in nikud for item in sublist]\n",
    "\n",
    "        # replace empty strings with <no_nikud> token\n",
    "        nikud = [label if label != \"\" else \"<no_nikud>\" for label in nikud]\n",
    "        # replace labels with ids\n",
    "        nikud = [self.label_to_id[label] for label in nikud]\n",
    "\n",
    "        # pad if needed\n",
    "        nikud = nikud + [len(label_to_id)] * (self.max_len - len(nikud))\n",
    "\n",
    "        # convert to tensor\n",
    "        nikud = torch.tensor(nikud, dtype=torch.long)\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'nikud': nikud,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "\n",
    "# create pytorch dataset class for punctuation restoration (returns input(text) and target(nikud))\n",
    "dataset = PunctuationRestorationDataset(data_df, alephbert_tokenizer, label_to_id, 100)\n",
    "sample = dataset[0]\n",
    "print(len(dataset))\n",
    "print(sample.keys())\n",
    "# remove spaces from text\n",
    "print(sample['text'].replace(\" \", \"\"))\n",
    "print(len(sample['text'].replace(\" \", \"\")))\n",
    "print((sample['nikud']))\n",
    "print(sample['input_ids'].shape)\n",
    "print(sample['attention_mask'].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split to train,val and test datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset to train, val and test\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# create dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# performe EDAs on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA on train, val and test dataset\n",
    "print(\"train dataset size: \", len(train_dataset))\n",
    "print(\"val dataset size: \", len(val_dataset))\n",
    "print(\"test dataset size: \", len(test_dataset))\n",
    "\n",
    "# train dataset\n",
    "print(\"train dataset\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the two models (one with look-ahead, one without)\n",
    "\n",
    "### train the two models\n",
    "\n",
    "### Evaluate the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full-Sentence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a charachter level transformer model with the following architecture:\n",
    "# 1. Embedding layer\n",
    "# 2. Transformer layer\n",
    "# 3. Fully connected layer\n",
    "\n",
    "model_path = 'C:\\\\Users\\\\baruc\\\\PycharmProjects\\\\pythonProject\\\\Punctuation_Restoration\\\\AlephBERT-main\\\\AlephBERT-main\\\\models\\\\alephbert-base'\n",
    "alephbert_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Assuming `your_vocab` is a list of tokens in your vocabulary\n",
    "alephbert_vocab = alephbert_tokenizer.get_vocab()\n",
    "\n",
    "# Your specific vocab size and embedding dimension\n",
    "new_vocab_size = alephbert_tokenizer.vocab_size\n",
    "embedding_dim = 512\n",
    "\n",
    "class FullSentenceModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=512, nhead=8, num_layers=6, output_dim=len(label_to_id), dropout=0.1):\n",
    "        super(FullSentenceModel, self).__init__()\n",
    "        \n",
    "        # Non-pretrained embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.transformer = nn.Transformer(d_model=embedding_dim, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers, dropout=dropout, activation='relu')\n",
    "        \n",
    "        # Fully connected layer with ReLU activation function\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, output_dim)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # Pass input through embedding layer\n",
    "        characters_embeddings = self.embedding(sentence)\n",
    "        \n",
    "        # Pass through transformer\n",
    "        characters_after_transformer_layers = self.transformer(characters_embeddings, characters_embeddings)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        predictions = self.fc(characters_after_transformer_layers)\n",
    "        \n",
    "        return predictions\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "vocab_size = len(alephbert_tokenizer.vocab)\n",
    "\n",
    "#scheduler = StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=len(label_to_id)).to(device)\n",
    "#weights = torch.tensor(list(label_weights.values())).to(device)\n",
    "#criterion = nn.CrossEntropyLoss(ignore_index=len(label_to_id), weight=weights).to(device)\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "# define the training loop\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['nikud'].to(device)\n",
    "        predictions = model.forward(input_ids)\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        labels = labels.view(-1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss_history.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# define the evaluation loop\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['nikud'].to(device)\n",
    "            predictions = model.forward(input_ids)\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            labels = labels.view(-1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# define the training and evaluation loop\n",
    "def train_and_evaluate(model, train_dataloader, val_dataloader, optimizer, criterion, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train(model, train_dataloader, optimizer, criterion)\n",
    "        #scheduler.step()\n",
    "        val_loss = evaluate(model, val_dataloader, criterion)\n",
    "        print(f'Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        # plot loss_history\n",
    "        plt.plot(loss_history)\n",
    "        plt.title('Loss history')\n",
    "        plt.ylabel('Loss value')\n",
    "        plt.xlabel('Batch #')\n",
    "        plt.show()\n",
    "\n",
    "def hyperparameter_grid_search():\n",
    "    # train the model\n",
    "    nhead = [2, 4, 8]\n",
    "    nlayers = [2, 4, 6]\n",
    "    lr = [0.0001, 0.00001]\n",
    "    for n in nhead:\n",
    "        for l in nlayers:\n",
    "            for r in lr:\n",
    "                model = FullSentenceModel(vocab_size, nhead=n, num_layers=l).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=r)\n",
    "                print(f'nhead: {n}, nlayers: {l}, lr: {r}')\n",
    "                train_and_evaluate(model, train_loader, val_loader, optimizer, criterion, epochs=5)\n",
    "\n",
    "# Imprortant reminder: Optimizer should be defined for each model separately\n",
    "model = FullSentenceModel(vocab_size, nhead=8, num_layers=6).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "train_and_evaluate(model, train_loader, val_loader, optimizer, criterion)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss_history\n",
    "plt.plot(loss_history)\n",
    "plt.title('Loss history')\n",
    "plt.ylabel('Loss value')\n",
    "plt.xlabel('Batch #')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    # predict on the first batch of the test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['nikud'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        predictions = model.forward(input_ids)\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        labels = labels.view(-1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        print(f'Loss: {loss.item():.4f}')\n",
    "        # cut the padding tokens\n",
    "        predictions = predictions[attention_mask.view(-1) == 1]\n",
    "        labels = labels[attention_mask.view(-1) == 1]\n",
    "        print(f'Predictions shape: {predictions.shape}')\n",
    "        print(f'Labels shape: {labels.shape}')\n",
    "        # argmax to get the best prediction\n",
    "        predictions = torch.argmax(predictions, dim=1)\n",
    "        print(f'Predictions vs labels: {predictions[:100]} vs {labels[:100]}')\n",
    "        # accuracy\n",
    "        accuracy = torch.sum(predictions == labels) / len(labels)\n",
    "        print(f'Accuracy: {accuracy.item():.4f}')\n",
    "        break\n",
    "\n",
    "\n",
    "# TODO: we notice we usually predict 2 (no_nikud) - why? how to fix?..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading-Direction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a skeleton, need to be fixed\n",
    "import torch.nn as nn\n",
    "\n",
    "class ReadingDirectionModel(nn.Module):\n",
    "    def __init__(self, char_vocab_size, char_embedding_dim, word_hidden_dim, char_hidden_dim, num_labels, word_lstm_layers=1, char_lstm_layers=4):\n",
    "        super(ReadingDirectionModel, self).__init__()\n",
    "        \n",
    "        # Character Embedding Layer\n",
    "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim)\n",
    "        \n",
    "        # Word-Level BiLSTM\n",
    "        self.word_bilstm = nn.LSTM(char_embedding_dim, word_hidden_dim, num_layers=word_lstm_layers, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Character-Level LSTM\n",
    "        self.char_lstm = nn.LSTM(char_embedding_dim + 2 * word_hidden_dim, char_hidden_dim, num_layers=char_lstm_layers, batch_first=True)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(char_hidden_dim, num_labels)\n",
    "        \n",
    "        # Dropout Layer\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, char_sequence, word_boundaries):\n",
    "        # Apply Character Embedding\n",
    "        char_embedded = self.char_embedding(char_sequence)\n",
    "        \n",
    "        # Process each word using the Word-Level BiLSTM\n",
    "        word_embeddings = []\n",
    "        for i, boundaries in enumerate(word_boundaries):\n",
    "            word_sequences = [char_embedded[i, start:end, :] for start, end in zip(boundaries[:-1], boundaries[1:])]\n",
    "            word_sequences = [self.word_bilstm(word_seq.unsqueeze(0))[0][:, -1, :] for word_seq in word_sequences]\n",
    "            word_embedding = torch.cat(word_sequences, dim=0)\n",
    "            word_embeddings.append(word_embedding)\n",
    "        \n",
    "        # Repeat word embeddings for each character\n",
    "        char_word_embeddings = [torch.repeat_interleave(word_embedding, boundary[1:] - boundary[:-1]) for word_embedding, boundary in zip(word_embeddings, word_boundaries)]\n",
    "        \n",
    "        # Concatenate character embeddings with word embeddings\n",
    "        char_word_embeddings = torch.stack(char_word_embeddings)\n",
    "        char_word_embedded = torch.cat((char_embedded, char_word_embeddings), dim=2)\n",
    "        \n",
    "        # Apply Character-Level LSTM\n",
    "        char_lstm_output, _ = self.char_lstm(char_word_embedded)\n",
    "        \n",
    "        # Apply Dropout\n",
    "        char_lstm_output = self.dropout(char_lstm_output)\n",
    "        \n",
    "        # Apply Fully Connected Layer\n",
    "        predictions = self.fc(char_lstm_output)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Hyperparameters (example)\n",
    "char_vocab_size = 128\n",
    "char_embedding_dim = 32\n",
    "word_hidden_dim = 16\n",
    "char_hidden_dim = 512\n",
    "num_labels = 15\n",
    "\n",
    "# Creating the model instance (example)\n",
    "reading_direction_model = ReadingDirectionModel(char_vocab_size, char_embedding_dim, word_hidden_dim, char_hidden_dim, num_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the dual model class, it will be composed of two models.\n",
    "#### whenever there is a disagreement between the two models, the model will add nikud using the lookahead model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the dual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
