{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "### here we need to load the data and extract only data with vowels punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "# path to the jason file for the dataset\n",
    "data_json_path = 'data/books.json'\n",
    "\n",
    "# Root directory where the downloaded files will be saved\n",
    "texts_path = 'data/texts'\n",
    "\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(texts_path):\n",
    "    os.makedirs(texts_path)\n",
    "\n",
    "\n",
    "# Load the json dataset\n",
    "with open(data_json_path, 'r', encoding='utf-8') as f:\n",
    "    jason_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# download the files and save them in a folder\n",
    "\n",
    "#### remove\\add the comment as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the json dataset and download the files\n",
    "for entry in tqdm(jason_data):\n",
    "    try:\n",
    "        # Download the Nikud Meteg file\n",
    "        if entry['fileName'] + '__nikud_meteg' in os.listdir(texts_path):\n",
    "            continue\n",
    "        nikud_meteg_url = entry['nikudMetegFileURL']\n",
    "        nikud_meteg_local_path = os.path.join(texts_path, entry['fileName'] + '__nikud_meteg.zip')\n",
    "        nikud_meteg_response = requests.get(nikud_meteg_url)\n",
    "        with open(nikud_meteg_local_path, 'wb') as f:\n",
    "            f.write(nikud_meteg_response.content)\n",
    "\n",
    "            # Unzip the Nikud Meteg file\n",
    "            with ZipFile(nikud_meteg_local_path, 'r') as zipObj:\n",
    "                zipObj.extractall(os.path.join(texts_path, entry['fileName'] + '__nikud_meteg'))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {entry['fileName']}: {e}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "# iterate through the texts folder and delete the zip folders\n",
    "for file in tqdm(os.listdir(texts_path)):\n",
    "    if file.endswith(\".zip\"):\n",
    "        os.remove(os.path.join(texts_path, file))\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author files\n",
    "\n",
    "### Create a dictionary whose keys are authors and values are a list containing all it's files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a method to create the author files dictionary\n",
    "def create_author_files_dict(author_files):\n",
    "    \"\"\"\n",
    "    This function creates a dictionary of author files with a list of their corresponding texts.\n",
    "    \"\"\"\n",
    "    author_files_dict = {}\n",
    "    for file in author_files:\n",
    "        author_files_dict[file] = []\n",
    "        for text_file_name in os.listdir(os.path.join(texts_path, file)):\n",
    "            if text_file_name.endswith('.txt'):\n",
    "                author_files_dict[file].append(text_file_name)\n",
    "    return author_files_dict\n",
    "\n",
    "author_files = os.listdir(texts_path)\n",
    "author_files_dict = create_author_files_dict(author_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a txt file from the author files dictionary\n",
    "def read_txt_file(file_path):\n",
    "    \"\"\"\n",
    "    This function reads a txt file and returns the text as a string.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def remove_nikud(string):\n",
    "    \"\"\"Removes the nikud from the given string.\"\"\"\n",
    "    nikud = re.compile(r'[\\u05B0-\\u05C2]') # Nikud unicode range (https://en.wikipedia.org/wiki/Unicode_and_HTML_for_the_Hebrew_alphabet)\n",
    "    # negate the nikud regex\n",
    "    return nikud.sub(\"\", string)\n",
    "\n",
    "def get_nikud(word):\n",
    "    \"\"\"Returns the nikud from the given word.\"\"\"\n",
    "    nikud = re.compile(r'[\\u05B0-\\u05C2]') # Nikud unicode range (https://en.wikipedia.org/wiki/Unicode_and_HTML_for_the_Hebrew_alphabet)\n",
    "    # check if each element in the array is nikud\n",
    "    current_nikud = ''\n",
    "    nikud_arr = []\n",
    "    for i in range(len(word)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if nikud.match(word[i]):\n",
    "            current_nikud += word[i]\n",
    "        else:\n",
    "            nikud_arr.append(current_nikud)\n",
    "            current_nikud = ''\n",
    "    nikud_arr.append(current_nikud)\n",
    "    return nikud_arr\n",
    "\n",
    "def add_nikud(word, nikud):\n",
    "    \"\"\"Adds the nikud to the given word.\"\"\"\n",
    "    new_word = ''\n",
    "    for i in range(len(word)):\n",
    "        new_word += word[i] + nikud[i]\n",
    "    return new_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create DataSet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset class, get iten will return the text and the nikud_meteg\n",
    "class NikudMetegDataset(Dataset):\n",
    "    def __init__(self, files_list):\n",
    "        self.files_list = files_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read the text and the nikud_meteg files\n",
    "        with open(self.files_list[idx][0], 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        with open(self.files_list[idx][1], 'r', encoding='utf-8') as f:\n",
    "            nikud_meteg = f.read()\n",
    "        return text, nikud_meteg\n",
    "\n",
    "\n",
    "\n",
    "# split the data into train validation and test\n",
    "train_files_list, test_files_list = train_test_split(files_tuple_list, test_size=0.1, random_state=42)\n",
    "train_files_list, val_files_list = train_test_split(train_files_list, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a dataset object\n",
    "train_dataset = NikudMetegDataset(train_files_list)\n",
    "val_dataset = NikudMetegDataset(val_files_list)\n",
    "test_dataset = NikudMetegDataset(test_files_list)\n",
    "\n",
    "print(f\"Number of train files: {len(train_dataset)}\")\n",
    "print(f\"Number of validation files: {len(val_dataset)}\")\n",
    "print(f\"Number of test files: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# performe EDAs on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_eda(dataset, title):\n",
    "    # Get the text and nikud_meteg data from the dataset\n",
    "    texts = []\n",
    "    nikud_metegs = []\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        text, nikud_meteg = dataset[i]\n",
    "        texts.append(text)\n",
    "        nikud_metegs.append(nikud_meteg)\n",
    "\n",
    "    # Calculate the length of the texts and nikud_metegs\n",
    "    text_lengths = [len(text) for text in texts]\n",
    "    nikud_meteg_lengths = [len(nikud_meteg) for nikud_meteg in nikud_metegs]\n",
    "\n",
    "    # Calculate the mean and standard deviation of the text and nikud_meteg lengths\n",
    "    text_mean_length = sum(text_lengths) / len(text_lengths)\n",
    "    text_std_length = statistics.stdev(text_lengths)\n",
    "    nikud_meteg_mean_length = sum(nikud_meteg_lengths) / len(nikud_meteg_lengths)\n",
    "    nikud_meteg_std_length = statistics.stdev(nikud_meteg_lengths)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Number of samples in the dataset: {len(dataset)}\")\n",
    "    print(f\"Mean text length: {text_mean_length:.2f} (std: {text_std_length:.2f})\")\n",
    "    print(f\"Mean nikud_meteg length: {nikud_meteg_mean_length:.2f} (std: {nikud_meteg_std_length:.2f})\")\n",
    "\n",
    "    # Create histograms of the text and nikud_meteg lengths\n",
    "    plt.hist(text_lengths, bins=50)\n",
    "    plt.title(f\"{title} - Text Lengths\")\n",
    "    plt.xlabel(\"Length\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(nikud_meteg_lengths, bins=50)\n",
    "    plt.title(f\"{title} - Nikud Meteg Lengths\")\n",
    "    plt.xlabel(\"Length\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "    # Create a boxplot of the text and nikud_meteg lengths\n",
    "    plt.boxplot(text_lengths)\n",
    "    plt.title(f\"{title} - Text Lengths\")\n",
    "    plt.ylabel(\"Length\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.boxplot(nikud_meteg_lengths)\n",
    "    plt.title(f\"{title} - Nikud Meteg Lengths\")\n",
    "    plt.ylabel(\"Length\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Perform EDA on the train, validation and test datasets\n",
    "print(\"Train Dataset EDA:\")\n",
    "perform_eda(train_dataset, \"Train Dataset\")\n",
    "print(\"Validation Dataset EDA:\")\n",
    "perform_eda(val_dataset, \"Validation Dataset\")\n",
    "print(\"Test Dataset EDA:\")\n",
    "perform_eda(test_dataset, \"Test Dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataloader parameters\n",
    "batch_size = 1\n",
    "\n",
    "# Create the dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# test the dataloader\n",
    "for text, nikud_meteg in train_dataloader:\n",
    "    # print as list of chars\n",
    "    print((text[0]))\n",
    "    print((nikud_meteg[0]))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the two models (one with look-ahead, one without)\n",
    "\n",
    "### train the two models\n",
    "\n",
    "### Evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PunctuationPredictionModelWithLookahead(nn.Module):\n",
    "    def __init__(self, num_words, num_punctuations, embedding_dim=256, num_heads=8, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_words, embedding_dim)\n",
    "        self.transformer = nn.Transformer(embedding_dim, num_heads, num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, num_punctuations)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PunctuationPredictionModelWithoutLookahead(nn.Module):\n",
    "    def __init__(self, num_words, num_punctuations, embedding_dim=256, num_heads=8, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_words, embedding_dim)\n",
    "        self.transformer = nn.Transformer(embedding_dim, num_heads, num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, num_punctuations)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        mask = self._generate_future_mask(x.size(0)).to(x.device)\n",
    "        x = self.transformer(x, src_mask=mask)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def _generate_future_mask(self, size):\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the dual model class, it will be composed of two models.\n",
    "#### whenever there is a disagreement between the two models, the model will add nikud using the lookahead model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the dual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
