{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "### here we need to load the data and extract only data with vowels punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "# path to the jason file for the dataset\n",
    "data_json_path = 'data/books.json'\n",
    "\n",
    "# Root directory where the downloaded files will be saved\n",
    "texts_path = 'data/texts'\n",
    "\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(texts_path):\n",
    "    os.makedirs(texts_path)\n",
    "\n",
    "\n",
    "# Load the json dataset\n",
    "with open(data_json_path, 'r', encoding='utf-8') as f:\n",
    "    jason_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# download the files and save them in a folder\n",
    "\n",
    "#### remove\\add the comment as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loop through the json dataset and download the files\n",
    "# for entry in tqdm(jason_data):\n",
    "#     try:\n",
    "#         # Download the Nikud Meteg file\n",
    "#         if entry['fileName'] + '__nikud_meteg' in os.listdir(texts_path):\n",
    "#             continue\n",
    "#         nikud_meteg_url = entry['nikudMetegFileURL']\n",
    "#         nikud_meteg_local_path = os.path.join(texts_path, entry['fileName'] + '__nikud_meteg.zip')\n",
    "#         nikud_meteg_response = requests.get(nikud_meteg_url)\n",
    "#         with open(nikud_meteg_local_path, 'wb') as f:\n",
    "#             f.write(nikud_meteg_response.content)\n",
    "\n",
    "#             # Unzip the Nikud Meteg file\n",
    "#             with ZipFile(nikud_meteg_local_path, 'r') as zipObj:\n",
    "#                 zipObj.extractall(os.path.join(texts_path, entry['fileName'] + '__nikud_meteg'))\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading file {entry['fileName']}: {e}\")\n",
    "#         continue\n",
    "\n",
    "\n",
    "# # iterate through the texts folder and delete the zip folders\n",
    "# for file in tqdm(os.listdir(texts_path)):\n",
    "#     if file.endswith(\".zip\"):\n",
    "#         os.remove(os.path.join(texts_path, file))\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author files\n",
    "\n",
    "### Create a dictionary whose keys are authors and values are a list containing all it's files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a method to create the author files dictionary\n",
    "def create_author_files_dict(author_files):\n",
    "    \"\"\"\n",
    "    This function creates a dictionary of author files with a list of their corresponding texts.\n",
    "    \"\"\"\n",
    "    author_files_dict = {}\n",
    "    for file in author_files:\n",
    "        author_files_dict[file] = []\n",
    "        for text_file_name in os.listdir(os.path.join(texts_path, file)):\n",
    "            if text_file_name.endswith('.txt'):\n",
    "                author_files_dict[file].append(text_file_name)\n",
    "    return author_files_dict\n",
    "\n",
    "author_files = os.listdir(texts_path)\n",
    "author_files_dict = create_author_files_dict(author_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nikud unicode range (https://en.wikipedia.org/wiki/Unicode_and_HTML_for_the_Hebrew_alphabet)\n",
    "\n",
    "# Read a txt file from the author files dictionary\n",
    "def read_txt_file(file_path):\n",
    "    \"\"\"\n",
    "    This function reads a txt file and returns the text as a string.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def remove_nikud(string):\n",
    "    \"\"\"Removes the nikud from the given string.\"\"\"\n",
    "    nikud = re.compile(r'[\\u05B0-\\u05C2]')\n",
    "    return nikud.sub(\"\", string)\n",
    "\n",
    "def get_nikud(word):\n",
    "    \"\"\"Returns the nikud from the given word.\"\"\"\n",
    "    nikud = re.compile(r'[\\u05B0-\\u05C2]')\n",
    "    current_nikud = ''\n",
    "    nikud_arr = []\n",
    "    for i in range(len(word)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if nikud.match(word[i]):\n",
    "            current_nikud += word[i]\n",
    "        else:\n",
    "            nikud_arr.append(current_nikud)\n",
    "            current_nikud = ''\n",
    "    nikud_arr.append(current_nikud)\n",
    "    return nikud_arr\n",
    "\n",
    "def add_nikud(word, nikud):\n",
    "    \"\"\"Adds the nikud to the given word.\"\"\"\n",
    "    new_word = ''\n",
    "    for i in range(len(word)):\n",
    "        new_word += word[i] + nikud[i]\n",
    "    return new_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data to sentences and save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['פֶּתַח דָּבָר  יִתְבָּרֵךְ הַבּוֹרֵא וְיִשְׁתַּבַּח הַיּוֹצֵר אֲשֶׁר מֵעוּדַי גָּבַר עָלַי חַסְדּוֹ לָשׂוּם חֶלְקִי בֵּין חוֹבְשֵׁי ביהמ\"ד לְהִתְחַמֵּם כְּנֶגֶד אוּרָן שֶׁל חֲכָמִים', ' וְלֵיֽהָנוֹת מִזִּיו הַתּוֹרָה', ' וַאֲשֶׁר לֹא עָזַב חַסְדּוֹ זֶה מִמֶּנִּי עַד הַיּוֹם', ' וְהִיא שֶׁעָמְדָה לִי לְחַדֵּשׁ חִיֽדּוּשִׁים בְּעִנְיָנִים שׁוֹנִים בְּשַׁ\"ס וּפוֹסְקִים', \" כְּיָד ה' הַטּוֹבָה עָלַי\", ' וָתָ\"ל כִּי בְּכׇל מָקוֹם שָׂמְתִּי לִי לְמַטָּרָה', ' לְבָרֵר דִּין וַהֲלָכָה וַחֲקִירָה חֲדָשָׁה', ' אוֹ לַעֲמֹוֽד עַל בֵּיֽרוּר ד\\' הָרִאשׁוֹנִים זַ\"ל', ' וְלֹא לְפַלְפֵּל בְּפִלְפּוּל שֶׁל הֶבֶל לְאֵין קֵץ וְתַכְלִית', ' כַּאֲשֶׁר עֵינֶיךָ הַקּוֹרֵא תֶּחֱזֶינָה מֵישָׁרִים', ' וּמַה מְאֹד יָגֵל לִבִּי וְתַעֲלֹוֽזְנָה כִּלְיוֹתַי בְּצֵאתָם עַתָּה לְאוֹר עוֹלָם', ' וְיוּחֲקוּ בְּסֵפֶר מִלֵּי', ' זֶה הַיּוֹם שֶׁקִּוִּיתִי', ' כִּי יָפוּצוּ מֵעֵינוֹתַי הַחוּצָה', ' כִּי אׇמְנָם ת\"ל הַרְבֵּה מִדְּבָרַי הי\\' לְמַרְאֵה עֵינַי גְּדוֹלֵי יִשְׂרָאֵל יחי\\' אֲשֶׁר נִשֵּׂאתִי וְנָתַתִּי עִמָּהֶם בַּהֲלָכָה', ' וְהַרְבֵּה מֵהֶם וְאִיֽלּוּ בְּמִדַּת טוֹבָם וְעַנְוְתָנוּתָם לְכַבְּדֵנִי בִּתְשׁוּבוֹתֵיהֶם כַּאֲשֶׁר יִרְאֶה הָרוֹאֶה בִּפְנִים הַסֵּפֶר', ' וַאֲשֶׁר לָזֹאת לֹא מָצָאתִי לִי לְחוֹבָה לָקַחַת הַסְכָּמָה עלי\\' מִגְּאוֹנֵי דּוֹרֵנוּ הי\"ו כְּיֵאוֹת לְצָעִיר יָמִים כָּמוֹנִי הַיּוֹם', ' אַחֲרֵי אֲשֶׁר הַמַּשָּׂא וּמַתָּן שֶׁלָּהֶם עִמָּדִי וְכֵן הֶעָרוֹתֵיהֶם בִּדְבָרַי הֵן הֵם הַסְכָּמוֹתֵיהֶן', ' וַאֲנִי תְּפִלָּה', ' לְהַשּׁוֹכֵן בִּשְׁמֵי מַעֲלָה', ' כִּי בַּל יָסִיר חַסְדּוֹ מִמֶּנִּי סֶלָה', ' וְיַעְזְרֵנִי לְשֶׁבֶת תָּמִיד בְּאׇהֳלָהּ שֶׁל תּוֹרָה', 'וְלָלוּן בְּעֻוֽמְּקָה שֶׁל הֲלָכָה', ' כִּי זֹאת הִיא כׇּל תִּקְוָתִי וּמַשְׂאַת נַפְשִׁי כׇּל הַיָּמִים', ' וּכְשֵׁם שֶׁעֲזָרַנִי עַתָּה לְהוֹצִיא לָאוֹר אֶת בְּכוֹרֵי יְמֵי נְעוּרַי', ' כֵּן יְזַכֵּנִי וְיַעַזְרֵנִי לְהוֹסִיף אֹוֽמֶץ וְלַגְבֵּר חֲיָלִים בַּתּוֹרָה', ' וּלְהָפִיץ עוֹד מַעְיָנוֹת רַבּוֹת חוּצָה בְּטַעַם זְקֵנִים וְחָכְמַת יְשִׁישִׁים לְאַסּוֹקֵי', ' שְׁמַעְתָּא אַלִּיֽבָּא דְּהִלְכְתָא', ' וּלְכַוֵּיֽן לַאֲמִיֽתָּהּ שֶׁל תּוֹרָה', \" כִּי מַה מְאֹד אָהַבְתִּי לְצָלוּל בְּמַיִם אַדִּירִים בְּיַם הַתַּלְמוּד הק'\", ' מִהְיוֹת נִטְרָד בֵּין גַּלִּי הַיָּם הַזּוֹעֵף כִּסְפִינָה הַמְטֹוֽרֶפֶת', ' וְקָרָאתִי אֶת שֵׁם סִפְרִי זֶה אֲפִיקֵי יָם כִּי שָׁם זֶה אׇמְנָם יָאוּת לַקֹּוֽבֶץ עִנְיָנִים שׁוֹנִים בְּחִקְרֵי יָם', 'הַתַּלְמוּד', ' וְגַם שְׁמִי בְּקִרְבּוֹ כִּי \"יָם\" הוּא ר\"ת שְׁמִי יְחִיאֵל מִיכַל וְגַם שֵׁם כְּבוֹד אאמו\"ר הָרַב הַגָּדוֹל בתו\"י וכו\\' מ\\' ר\\' מֶרִרְכֵי יוֹנָה שְׁלִיטָ\"א ראבינאווץ מִסְּלוֹצָק בְּהָרַב הג\\' המאה\"ג ר\\' יְחִיאֵל מִיכַל זללה\"ה אב\"ד בְּאוּרִיטְשָׁא ושׁינאווקא', ' אֲשֶׁר מִיַּלְדוּתִי גְּדֵלֵנִי עַל בִּרְכֵּי הַתּוֹרָה', ' וְכׇל תִּקְוָתוֹ וּמַשְׂאַת נַפְשׁוֹ הָיְתָה לִרְאוֹת אוֹתִי כְּגֶפֶן שֶׁתּוֹלָה עַל פַּלְגֵי מֵי יַם הַתַּלְמוּד', ' יַאֲרִיךְ ה\\' יָמָיו וּשְׁנוֹתָיו וְיָנוּב בְּשֵׂיבָה טוֹבָה או\"א: וּבַחֲתִימַת דְּבָרִי עוֹד חוֹבָה עָלַי לְבַקֵּשׁ מָטוּ מִכְּבוֹד גְּדוֹלֵי הַדּוֹר הי\"ו אֲשֶׁר דִּבְרֵיהֶם אֵלַי בָּאוּ בְּסִפְרֵי', 'זֶה וְלִפְעָמִים אֲנִי מֵשִׁיב עֲלֵיהֶם וְיֵשׁ בְּדִבְרֵי תְּשׁוּבוֹתַי דְּבָרִים שֶׁעֲדַיִיֽן לֹא הָיוּ לְמַרְאֵה עֵינֵיהֶם', ' כִּי חָלִילָה לֹא לְהָקֵל בִּכְבוֹדָם עָשִׂיתִי זֹאת', ' רַק בַּאֲשֶׁר לֹא חָפַצְתִּי לְהַטְרִיחָם יוֹתֵר מִדַּאי לָבוֹא עוֹד בִּכְתוּבִים: וְהִנְנִי חוֹתֵם מֵעֵין פְּתִיחָה בְּשֶׁבַח וְהוֹדָיָ\\' לַבַּעַל הַגְּמוּלוֹת ב\"ה', ' אֲשֶׁר הַנָּעִים גּוֹרָלִי לְנֶחְלֵנִי עַל מֵי מְנוּחוֹת', 'התוה\"ק וֶהֱבִיאַנִי בֵּית חוֹתְנִי הָרַב הַגְּבִיר הַנִּכְבָּד רָחֵים וּמוֹקִיר רַבָּנָן ר\\' אַבְרָהָם יַעֲקֹב הַכֹּהֵן שְׁלִיטָ\"א קאפלאן בְּעִיר לעכאוויטשׁ הַמַּחֲזִיק בְּיָדִי וּמְחַלֵּק לִי מֵעֲסָקָיו הַגְּדוֹלִים לְמַעַן אוּכַל לְהַגּוֹת בַּתּוֹרָה בְּאֵין מִפְּרִיע וְאָתוּ בְּדוֹמֶה לוֹ בִּתּוֹ רַעְיָתִי הַכְּבוּדָּה מ\\' ליעבע תחי\\'', \" אֲשֶׁר גַּם הִיא בְּכׇל לִבָּהּ מִתְאַמֶּצֶת לְהָסִיר מִמֶּנִּי כׇּל מַפְרִיעַ לְמַעַן אהי' פָּנוּי לְתוֹרַת ה'\", ' יַאֲרִיךְ ה\\' יְמֵי כְּבוֹד מַר חוֹתְנִי שְׁלִיטָ\"א כִּי יָנוּב בְּשֵׂיבָה טוֹבָה דֶּשֶׁן וְרַעֲנָן לְאֹוֽרֶךְ יָמִים', \" וּתְבֹוֽרַךְ מִנָּשִׁים רַעְיָתִי תחי'\", ' הֲלֹא כֹּה דִּבְרֵי הַמַּעֲתִיר בְּעַד שְׁלֹוֽמָם וְטוֹבָם וְאׇשְׁרָם כׇּל הַיָּמִים', ' יְחִיאֵל מִיכַל רַאבִּינָאוִויץ מִלַּעַכְאוּוִיטַשׁ', '', '(יְלִיד סְלֹוֽצֶק)', 'לְזֵכֶר עוֹלָם', 'יְהִי שֵׁם אִמִּי מָרְתִי הַצְּנוּעָה מ\\' יענטא זללה\"ה שֶׁהָלְכָה לְעוֹלָמָהּ יוֹם ב\\' י\"ז טֵבֵת תרנ\"ה', ' יְהִי זָכְרָה', 'לִבְרָכָה לְעוֹלָמִים כְּמַאֲמָר רז\"ל (בְּרָכוֹת דַּף י\"ז) הָנֵי נָשֵׁי בְּמַאי זָכְיָיֽן בְּאַקְרוֹיֵי בְּנַיְיֽהוּ לְבֵי כְנִישְׁתָּא:']\n",
      "פֶּתַח דָּבָר  יִתְבָּרֵךְ הַבּוֹרֵא וְיִשְׁתַּבַּח הַיּוֹצֵר אֲשֶׁר מֵעוּדַי גָּבַר עָלַי חַסְדּוֹ לָשׂוּם חֶלְקִי בֵּין חוֹבְשֵׁי ביהמ\"ד לְהִתְחַמֵּם כְּנֶגֶד אוּרָן שֶׁל חֲכָמִים. וְלֵיֽהָנוֹת מִזִּיו הַתּוֹרָה. וַאֲשֶׁר לֹא עָזַב חַסְדּוֹ זֶה מִמֶּנִּי עַד הַיּוֹם. וְהִיא שֶׁעָמְדָה לִי לְחַדֵּשׁ חִיֽדּוּשִׁים בְּעִנְיָנִים שׁוֹנִים בְּשַׁ\"ס וּפוֹסְקִים. כְּיָד ה' הַטּוֹבָה עָלַי. וָתָ\"ל כִּי בְּכׇל מָקוֹם שָׂמְתִּי לִי לְמַטָּרָה. לְבָרֵר דִּין וַהֲלָכָה וַחֲקִירָה חֲדָשָׁה. אוֹ לַעֲמֹוֽד עַל בֵּיֽרוּר ד' הָרִאשׁוֹנִים זַ\"ל. וְלֹא לְפַלְפֵּל בְּפִלְפּוּל שֶׁל הֶבֶל לְאֵין קֵץ וְתַכְלִית. כַּאֲשֶׁר עֵינֶיךָ הַקּוֹרֵא תֶּחֱזֶינָה מֵישָׁרִים. וּמַה מְאֹד יָגֵל לִבִּי וְתַעֲלֹוֽזְנָה כִּלְיוֹתַי בְּצֵאתָם עַתָּה לְאוֹר עוֹלָם. וְיוּחֲקוּ בְּסֵפֶר מִלֵּי. זֶה הַיּוֹם שֶׁקִּוִּיתִי. כִּי יָפוּצוּ מֵעֵינוֹתַי הַחוּצָה. כִּי אׇמְנָם ת\"ל הַרְבֵּה מִדְּבָרַי הי' לְמַרְאֵה עֵינַי גְּדוֹלֵי יִשְׂרָאֵל יחי' אֲשֶׁר נִשֵּׂאתִי וְנָתַתִּי עִמָּהֶם בַּהֲלָכָה. וְהַרְבֵּה מֵהֶם וְאִיֽלּוּ בְּמִדַּת טוֹבָם וְעַנְוְתָנוּתָם לְכַבְּדֵנִי בִּתְשׁוּבוֹתֵיהֶם כַּאֲשֶׁר יִרְאֶה הָרוֹאֶה בִּפְנִים הַסֵּפֶר. וַאֲשֶׁר לָזֹאת לֹא מָצָאתִי לִי לְחוֹבָה לָקַחַת הַסְכָּמָה עלי' מִגְּאוֹנֵי דּוֹרֵנוּ הי\"ו כְּיֵאוֹת לְצָעִיר יָמִים כָּמוֹנִי הַיּוֹם. אַחֲרֵי אֲשֶׁר הַמַּשָּׂא וּמַתָּן שֶׁלָּהֶם עִמָּדִי וְכֵן הֶעָרוֹתֵיהֶם בִּדְבָרַי הֵן הֵם הַסְכָּמוֹתֵיהֶן. וַאֲנִי תְּפִלָּה. לְהַשּׁוֹכֵן בִּשְׁמֵי מַעֲלָה. כִּי בַּל יָסִיר חַסְדּוֹ מִמֶּנִּי סֶלָה. וְיַעְזְרֵנִי לְשֶׁבֶת תָּמִיד בְּאׇהֳלָהּ שֶׁל תּוֹרָה\n",
      "וְלָלוּן בְּעֻוֽמְּקָה שֶׁל הֲלָכָה. כִּי זֹאת הִיא כׇּל תִּקְוָתִי וּמַשְׂאַת נַפְשִׁי כׇּל הַיָּמִים. וּכְשֵׁם שֶׁעֲזָרַנִי עַתָּה לְהוֹצִיא לָאוֹר אֶת בְּכוֹרֵי יְמֵי נְעוּרַי. כֵּן יְזַכֵּנִי וְיַעַזְרֵנִי לְהוֹסִיף אֹוֽמֶץ וְלַגְבֵּר חֲיָלִים בַּתּוֹרָה. וּלְהָפִיץ עוֹד מַעְיָנוֹת רַבּוֹת חוּצָה בְּטַעַם זְקֵנִים וְחָכְמַת יְשִׁישִׁים לְאַסּוֹקֵי. שְׁמַעְתָּא אַלִּיֽבָּא דְּהִלְכְתָא. וּלְכַוֵּיֽן לַאֲמִיֽתָּהּ שֶׁל תּוֹרָה. כִּי מַה מְאֹד אָהַבְתִּי לְצָלוּל בְּמַיִם אַדִּירִים בְּיַם הַתַּלְמוּד הק'. מִהְיוֹת נִטְרָד בֵּין גַּלִּי הַיָּם הַזּוֹעֵף כִּסְפִינָה הַמְטֹוֽרֶפֶת. וְקָרָאתִי אֶת שֵׁם סִפְרִי זֶה אֲפִיקֵי יָם כִּי שָׁם זֶה אׇמְנָם יָאוּת לַקֹּוֽבֶץ עִנְיָנִים שׁוֹנִים בְּחִקְרֵי יָם\n",
      "הַתַּלְמוּד. וְגַם שְׁמִי בְּקִרְבּוֹ כִּי \"יָם\" הוּא ר\"ת שְׁמִי יְחִיאֵל מִיכַל וְגַם שֵׁם כְּבוֹד אאמו\"ר הָרַב הַגָּדוֹל בתו\"י וכו' מ' ר' מֶרִרְכֵי יוֹנָה שְׁלִיטָ\"א ראבינאווץ מִסְּלוֹצָק בְּהָרַב הג' המאה\"ג ר' יְחִיאֵל מִיכַל זללה\"ה אב\"ד בְּאוּרִיטְשָׁא ושׁינאווקא. אֲשֶׁר מִיַּלְדוּתִי גְּדֵלֵנִי עַל בִּרְכֵּי הַתּוֹרָה. וְכׇל תִּקְוָתוֹ וּמַשְׂאַת נַפְשׁוֹ הָיְתָה לִרְאוֹת אוֹתִי כְּגֶפֶן שֶׁתּוֹלָה עַל פַּלְגֵי מֵי יַם הַתַּלְמוּד. יַאֲרִיךְ ה' יָמָיו וּשְׁנוֹתָיו וְיָנוּב בְּשֵׂיבָה טוֹבָה או\"א: וּבַחֲתִימַת דְּבָרִי עוֹד חוֹבָה עָלַי לְבַקֵּשׁ מָטוּ מִכְּבוֹד גְּדוֹלֵי הַדּוֹר הי\"ו אֲשֶׁר דִּבְרֵיהֶם אֵלַי בָּאוּ בְּסִפְרֵי\n",
      "זֶה וְלִפְעָמִים אֲנִי מֵשִׁיב עֲלֵיהֶם וְיֵשׁ בְּדִבְרֵי תְּשׁוּבוֹתַי דְּבָרִים שֶׁעֲדַיִיֽן לֹא הָיוּ לְמַרְאֵה עֵינֵיהֶם. כִּי חָלִילָה לֹא לְהָקֵל בִּכְבוֹדָם עָשִׂיתִי זֹאת. רַק בַּאֲשֶׁר לֹא חָפַצְתִּי לְהַטְרִיחָם יוֹתֵר מִדַּאי לָבוֹא עוֹד בִּכְתוּבִים: וְהִנְנִי חוֹתֵם מֵעֵין פְּתִיחָה בְּשֶׁבַח וְהוֹדָיָ' לַבַּעַל הַגְּמוּלוֹת ב\"ה. אֲשֶׁר הַנָּעִים גּוֹרָלִי לְנֶחְלֵנִי עַל מֵי מְנוּחוֹת\n",
      "התוה\"ק וֶהֱבִיאַנִי בֵּית חוֹתְנִי הָרַב הַגְּבִיר הַנִּכְבָּד רָחֵים וּמוֹקִיר רַבָּנָן ר' אַבְרָהָם יַעֲקֹב הַכֹּהֵן שְׁלִיטָ\"א קאפלאן בְּעִיר לעכאוויטשׁ הַמַּחֲזִיק בְּיָדִי וּמְחַלֵּק לִי מֵעֲסָקָיו הַגְּדוֹלִים לְמַעַן אוּכַל לְהַגּוֹת בַּתּוֹרָה בְּאֵין מִפְּרִיע וְאָתוּ בְּדוֹמֶה לוֹ בִּתּוֹ רַעְיָתִי הַכְּבוּדָּה מ' ליעבע תחי'. אֲשֶׁר גַּם הִיא בְּכׇל לִבָּהּ מִתְאַמֶּצֶת לְהָסִיר מִמֶּנִּי כׇּל מַפְרִיעַ לְמַעַן אהי' פָּנוּי לְתוֹרַת ה'. יַאֲרִיךְ ה' יְמֵי כְּבוֹד מַר חוֹתְנִי שְׁלִיטָ\"א כִּי יָנוּב בְּשֵׂיבָה טוֹבָה דֶּשֶׁן וְרַעֲנָן לְאֹוֽרֶךְ יָמִים. וּתְבֹוֽרַךְ מִנָּשִׁים רַעְיָתִי תחי'. הֲלֹא כֹּה דִּבְרֵי הַמַּעֲתִיר בְּעַד שְׁלֹוֽמָם וְטוֹבָם וְאׇשְׁרָם כׇּל הַיָּמִים. יְחִיאֵל מִיכַל רַאבִּינָאוִויץ מִלַּעַכְאוּוִיטַשׁ.\n",
      "(יְלִיד סְלֹוֽצֶק)\n",
      "לְזֵכֶר עוֹלָם\n",
      "יְהִי שֵׁם אִמִּי מָרְתִי הַצְּנוּעָה מ' יענטא זללה\"ה שֶׁהָלְכָה לְעוֹלָמָהּ יוֹם ב' י\"ז טֵבֵת תרנ\"ה. יְהִי זָכְרָה\n",
      "לִבְרָכָה לְעוֹלָמִים כְּמַאֲמָר רז\"ל (בְּרָכוֹת דַּף י\"ז) הָנֵי נָשֵׁי בְּמַאי זָכְיָיֽן בְּאַקְרוֹיֵי בְּנַיְיֽהוּ לְבֵי כְנִישְׁתָּא:\n"
     ]
    }
   ],
   "source": [
    "text = read_txt_file(os.path.join(texts_path, 'afikeiyam1__nikud_meteg', 'afikeiyam1-002__nikud_meteg.txt'))\n",
    "text_after_split = re.split(r'\\n|\\.', text)\n",
    "print(text_after_split)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a unified csv of all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "columns = ['text_with_nikud', 'text_without_nikud', 'nikud', 'author', 'file_name', 'sentence_num']\n",
    "# with open('data/full_data.csv', 'w', encoding='utf-8') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow(columns)\n",
    "#     for author in tqdm(author_files_dict):\n",
    "#         for file in author_files_dict[author]:\n",
    "#             text = read_txt_file(os.path.join(texts_path, author, file))\n",
    "#             # split the text into sentences by \\n or .\n",
    "#             sentences = re.split(r'\\n|\\.', text)\n",
    "#             for i, sentence in enumerate(sentences):\n",
    "#                 sentence_words = sentence.split()\n",
    "#                 # remove the nikud from the sentence\n",
    "#                 sentence_without_nikud = remove_nikud(sentence)\n",
    "#                 # get the nikud from the sentence\n",
    "#                 nikud = list(map(get_nikud, sentence_words))\n",
    "\n",
    "#                 # add the sentence to the dataframe\n",
    "#                 writer.writerow([sentence, sentence_without_nikud, nikud, author, file, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     text_with_nikud   \n",
      "0  פֶּתַח דָּבָר  יִתְבָּרֵךְ הַבּוֹרֵא וְיִשְׁתּ...  \\\n",
      "1                     וְלֵיֽהָנוֹת מִזִּיו הַתּוֹרָה   \n",
      "2   וַאֲשֶׁר לֹא עָזַב חַסְדּוֹ זֶה מִמֶּנִּי עַד...   \n",
      "3   וְהִיא שֶׁעָמְדָה לִי לְחַדֵּשׁ חִיֽדּוּשִׁים...   \n",
      "4                          כְּיָד ה' הַטּוֹבָה עָלַי   \n",
      "\n",
      "                                  text_without_nikud   \n",
      "0  פתח דבר  יתברך הבורא וישתבח היוצר אשר מעודי גב...  \\\n",
      "1                                 וליהנות מזיו התורה   \n",
      "2                   ואשר לא עזב חסדו זה ממני עד היום   \n",
      "3   והיא שעמדה לי לחדש חידושים בענינים שונים בש\"ס...   \n",
      "4                                   כיד ה' הטובה עלי   \n",
      "\n",
      "                                               nikud                   author   \n",
      "0  [[ֶּ, ַ, ], [ָּ, ָ, ], [ִ, ְ, ָּ, ֵ, ְ], [ַ, ּ...  afikeiyam1__nikud_meteg  \\\n",
      "1  [[ְ, ֵ, ֽ, ָ, , ֹ, ], [ִ, ִּ, , ], [ַ, ּ, ֹ, ָ...  afikeiyam1__nikud_meteg   \n",
      "2  [[ַ, ֲ, ֶׁ, ], [ֹ, ], [ָ, ַ, ], [ַ, ְ, ּ, ֹ], ...  afikeiyam1__nikud_meteg   \n",
      "3  [[ְ, ִ, , ], [ֶׁ, ָ, ְ, ָ, ], [ִ, ], [ְ, ַ, ֵּ...  afikeiyam1__nikud_meteg   \n",
      "4        [[ְּ, ָ, ], [, ], [ַ, ּ, ֹ, ָ, ], [ָ, ַ, ]]  afikeiyam1__nikud_meteg   \n",
      "\n",
      "                         file_name  sentence_num  \n",
      "0  afikeiyam1-002__nikud_meteg.txt             0  \n",
      "1  afikeiyam1-002__nikud_meteg.txt             1  \n",
      "2  afikeiyam1-002__nikud_meteg.txt             2  \n",
      "3  afikeiyam1-002__nikud_meteg.txt             3  \n",
      "4  afikeiyam1-002__nikud_meteg.txt             4  \n"
     ]
    }
   ],
   "source": [
    "# dataframe of the CSV with chunksize of 1000\n",
    "# data_df_chunks = pd.read_csv('data/full_data.csv', chunksize=1000)\n",
    "\n",
    "# read only first 100000 rows of the CSV, we will use this for now\n",
    "data_df = pd.read_csv('data/full_data.csv', nrows=100000, converters={'nikud': eval})\n",
    "print(data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:04<00:00, 20122.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ֶּ': 0, 'ַ': 1, '<no_nikud>': 2, 'ָּ': 3, 'ָ': 4, 'ִ': 5, 'ְ': 6, 'ֵ': 7, 'ּ': 8, 'ֹ': 9, 'ְׁ': 10, 'ַּ': 11, 'ֲ': 12, 'ֶׁ': 13, 'ׂ': 14, 'ֶ': 15, 'ֵּ': 16, 'ֵׁ': 17, 'ְּ': 18, 'ֽ': 19, 'ִּ': 20, 'ׁ': 21, 'ִׁ': 22, 'ַׁ': 23, 'ָׂ': 24, 'ָׁ': 25, 'ֱ': 26, 'ְׂ': 27, 'ֵּׂ': 28, 'ָּׂ': 29, 'ּׁ': 30, 'ֳ': 31, 'ֻ': 32, 'ֹּ': 33, 'ֵׂ': 34, 'ִׂ': 35, 'ֶׂ': 36, 'ֵּׁ': 37, 'ַּׁ': 38, 'ְּׁ': 39, 'ֻּ': 40, 'ִּׁ': 41, 'ָּׁ': 42, 'ֻׁ': 43, 'ִּׂ': 44, 'ֹׁ': 45, 'ֶּׁ': 46, 'ַּׂ': 47, 'ַׂ': 48, 'ּׂ': 49, 'ֳּ': 50, 'ְּׂ': 51, 'ֻּׁ': 52, 'ֶּׂ': 53, 'ַָ': 54, 'ֲּ': 55, 'ֹׂ': 56, 'ַָ': 57, 'ֹּׁ': 58, 'ִּֿ': 59, 'ִַ': 60, 'ָֹ': 61, 'ִָ': 62, 'ֹּׂ': 63, 'ֻׂ': 64, 'ֲׁ': 65, 'ִָ': 66, 'ִַ': 67, 'ְִ': 68, 'ֱּׁ': 69, 'ֳׁ': 70, 'ַַ': 71, 'ֱּ': 72, 'ֳּׁ': 73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "label_to_id = {}\n",
    "id_to_label = {}\n",
    "for label_list in tqdm(data_df['nikud']):\n",
    "    # flatten the list\n",
    "    label_list = [item for sublist in label_list for item in sublist]\n",
    "    for label in label_list:\n",
    "        if len(label) == 0:\n",
    "            label = \"<no_nikud>\"\n",
    "        if label not in label_to_id:\n",
    "            label_to_id[label] = len(label_to_id)\n",
    "            id_to_label[len(id_to_label)] = label\n",
    "\n",
    "print(label_to_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download tokenizer and model\n",
    "(alephbert-base, with vocab of words with len <= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "model_path = 'C:\\\\Users\\\\baruc\\\\PycharmProjects\\\\pythonProject\\\\Punctuation_Restoration\\\\AlephBERT-main\\\\AlephBERT-main\\\\models\\\\alephbert-base'\n",
    "alephbert_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "alephbert_model = AutoModelForMaskedLM.from_pretrained(\"onlplab/alephbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "בדיקה של הדבר הזה\n",
      "tokenized:  ['ב', '##ד', '##י', '##ק', '##ה', 'ש', '##ל', 'ה', '##ד', '##ב', '##ר', 'ה', '##ז', '##ה']\n",
      "encoded:  [2, 177, 1039, 1008, 1013, 1016, 201, 1009, 180, 1039, 1037, 1014, 180, 1075, 1016, 3]\n",
      "decoded:  [CLS] בדיקה של הדבר הזה [SEP]\n"
     ]
    }
   ],
   "source": [
    "# test the tokenization and detokenization\n",
    "test = \"בדיקה של הדבר הזה\"\n",
    "tokenized = alephbert_tokenizer.tokenize(test)\n",
    "encoded = alephbert_tokenizer.encode(test)\n",
    "decoded = alephbert_tokenizer.decode(encoded)\n",
    "print(test)\n",
    "print(\"tokenized: \", tokenized)\n",
    "print(\"encoded: \", encoded)\n",
    "print(\"decoded: \", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create DataSet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "dict_keys(['text', 'nikud', 'input_ids', 'attention_mask'])\n",
      "88\n",
      "100\n",
      "torch.Size([100])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# create pytorch dataset class for punctuation restoration (returns input(text) and target(nikud))\n",
    "\n",
    "class PunctuationRestorationDataset(Dataset):\n",
    "    def __init__(self, data_df, tokenizer, label_to_id, max_len):\n",
    "        self.data = data_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label_to_id = label_to_id \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index): # TODO: need to make sure not to look at fraction of words\n",
    "        text = self.data.iloc[index]['text_without_nikud']\n",
    "        nikud = self.data.iloc[index]['nikud']  # list of lists of nikud\n",
    "\n",
    "        # flatten nikud list\n",
    "        nikud = [item for sublist in nikud for item in sublist]\n",
    "\n",
    "        # replace empty strings with <no_nikud> token\n",
    "        nikud = [label if label != \"\" else \"<no_nikud>\" for label in nikud]\n",
    "        # replace labels with ids\n",
    "        nikud = [self.label_to_id[label] for label in nikud]\n",
    "        # truncate if needed\n",
    "        nikud = nikud[:self.max_len]\n",
    "        # pad if needed\n",
    "        nikud = nikud + [len(label_to_id)] * (self.max_len - len(nikud))\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'nikud': nikud,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "\n",
    "# create pytorch dataset class for punctuation restoration (returns input(text) and target(nikud))\n",
    "dataset = PunctuationRestorationDataset(data_df, alephbert_tokenizer, label_to_id, 100)\n",
    "sample = dataset[0]\n",
    "print(len(dataset))\n",
    "print(sample.keys())\n",
    "# remove spaces from text\n",
    "print(len(sample['text'].replace(\" \", \"\")))\n",
    "print(len(sample['nikud']))\n",
    "print(sample['input_ids'].shape)\n",
    "print(sample['attention_mask'].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# split dataset to train, val and test\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# create dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# performe EDAs on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the two models (one with look-ahead, one without)\n",
    "\n",
    "### train the two models\n",
    "\n",
    "### Evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the dual model class, it will be composed of two models.\n",
    "#### whenever there is a disagreement between the two models, the model will add nikud using the lookahead model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the dual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
