{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "from torch.nn import Transformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score , roc_auc_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import random\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "### here we need to load the data and extract only data with vowels punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the jason file for the dataset\n",
    "data_json_path = 'data/books.json'\n",
    "\n",
    "# Root directory where the downloaded files will be saved\n",
    "texts_path = 'data/texts'\n",
    "\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(texts_path):\n",
    "    os.makedirs(texts_path)\n",
    "\n",
    "\n",
    "# Load the json dataset\n",
    "with open(data_json_path, 'r', encoding='utf-8') as f:\n",
    "    jason_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# download the files and save them in a folder\n",
    "\n",
    "#### remove\\add the comment as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loop through the json dataset and download the files\n",
    "# for entry in tqdm(jason_data):\n",
    "#     try:\n",
    "#         # Download the Nikud Meteg file\n",
    "#         if entry['fileName'] + '__nikud_meteg' in os.listdir(texts_path):\n",
    "#             continue\n",
    "#         nikud_meteg_url = entry['nikudMetegFileURL']\n",
    "#         nikud_meteg_local_path = os.path.join(texts_path, entry['fileName'] + '__nikud_meteg.zip')\n",
    "#         nikud_meteg_response = requests.get(nikud_meteg_url)\n",
    "#         with open(nikud_meteg_local_path, 'wb') as f:\n",
    "#             f.write(nikud_meteg_response.content)\n",
    "\n",
    "#             # Unzip the Nikud Meteg file\n",
    "#             with ZipFile(nikud_meteg_local_path, 'r') as zipObj:\n",
    "#                 zipObj.extractall(os.path.join(texts_path, entry['fileName'] + '__nikud_meteg'))\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading file {entry['fileName']}: {e}\")\n",
    "#         continue\n",
    "\n",
    "\n",
    "# # iterate through the texts folder and delete the zip folders\n",
    "# for file in tqdm(os.listdir(texts_path)):\n",
    "#     if file.endswith(\".zip\"):\n",
    "#         os.remove(os.path.join(texts_path, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author files\n",
    "\n",
    "### Create a dictionary whose keys are authors and values are a list containing all it's files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a method to create the author files dictionary\n",
    "def create_author_files_dict(author_files):\n",
    "    \"\"\"\n",
    "    This function creates a dictionary of author files with a list of their corresponding texts.\n",
    "    \"\"\"\n",
    "    author_files_dict = {}\n",
    "    for file in author_files:\n",
    "        author_files_dict[file] = []\n",
    "        for text_file_name in os.listdir(os.path.join(texts_path, file)):\n",
    "            if text_file_name.endswith('.txt'):\n",
    "                author_files_dict[file].append(text_file_name)\n",
    "    return author_files_dict\n",
    "\n",
    "author_files = os.listdir(texts_path)\n",
    "author_files_dict = create_author_files_dict(author_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nikud unicode range (https://en.wikipedia.org/wiki/Unicode_and_HTML_for_the_Hebrew_alphabet)\n",
    "\n",
    "\n",
    "#dictionary of nikud to name\n",
    "nikud_dict = { 'ְ' : 'Sheva', 'ֱ' : 'Hataf Segol', 'ֲ' : 'Hataf Patah', 'ֳ' : 'Hataf Qamats', 'ִ' : 'Hiriq', 'ֵ' : 'Tseri', 'ֶ' : 'Segol', 'ַ' : 'Patah', 'ָ' : 'Qamats', 'ֹ' : 'Holam', 'ֻ' : 'Qubuts', 'ּ' : 'Dagesh', 'ֽ' : 'Siluk', '־' : 'Maqaf', 'ֿ' : 'Rafe', 'ׁ' : 'Shin Dot', 'ׂ' : 'Sin Dot', 'ׄ' : 'Upper Dot', 'ׅ' : 'Lower Dot', 'ׇ' : 'Point Meteg', 'װ' : 'Yiddish Double Vav', 'ױ' : 'Yiddish Vav Yod', 'ײ' : 'Yiddish Double Yod', '׳' : 'Geresh', '״' : 'Gershayim' }\n",
    "# make inverse dictionary\n",
    "nikud_dict_inv = {v: k for k, v in nikud_dict.items()}\n",
    "\n",
    "# Read a txt file from the author files dictionary\n",
    "def read_txt_file(file_path):\n",
    "    \"\"\"\n",
    "    This function reads a txt file and returns the text as a string.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def remove_nikud(string):\n",
    "    \"\"\"Removes the nikud from the given string.\"\"\"\n",
    "    nikud = re.compile(r'[\\u0591-\\u05C7]')\n",
    "    return nikud.sub(\"\", string)\n",
    "\n",
    "def get_nikud(word):\n",
    "    \"\"\"Returns the nikud from the given word.\"\"\"\n",
    "    nikud = re.compile(r'[\\u0591-\\u05C7]')\n",
    "    current_nikud = ''\n",
    "    nikud_arr = []\n",
    "    for i in range(len(word)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if nikud.match(word[i]):\n",
    "            current_nikud += word[i]\n",
    "        else:\n",
    "            nikud_arr.append(current_nikud)\n",
    "            current_nikud = ''\n",
    "    nikud_arr.append(current_nikud)\n",
    "    return nikud_arr\n",
    "\n",
    "def add_nikud(word, nikud):\n",
    "    \"\"\"Adds the nikud to the given word.\"\"\"\n",
    "    new_word = ''\n",
    "    for i in range(len(word)):\n",
    "        new_word += word[i] + nikud[i]\n",
    "    return new_word\n",
    "\n",
    "def add_nikud_to_text(text, nikud):\n",
    "    \"\"\"Adds the nikud to the given text.\"\"\"\n",
    "    new_text = ''\n",
    "    for word in text.split(' '):\n",
    "        new_text += add_nikud(word, nikud) + ' '\n",
    "    return new_text\n",
    "\n",
    "def remove_first_char_if_nikud(word):\n",
    "    \"\"\"Removes the first char of the word if it is nikud.\"\"\"\n",
    "    nikud = re.compile(r'[\\u0591-\\u05C7]')\n",
    "    if nikud.match(word[0]):\n",
    "        return word[1:]\n",
    "    return word\n",
    "\n",
    "def get_words_indices_from_text(text):\n",
    "    \"\"\"Returns the indices of the words in the given text.\"\"\"\n",
    "    text_words = text.split()\n",
    "    text_words_lengths = list(map(len, text_words))\n",
    "    text_words_indices = [(sum(text_words_lengths[:i]), sum(text_words_lengths[:i+1])-1) for i in range(len(text_words_lengths))]\n",
    "    return text_words_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test nikud functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_words: ['פֶּתַח', 'דָּבָר', 'יִתְבָּרֵךְ', 'הַבּוֹרֵא', 'וְיִשְׁתַּבַּח', 'הַיּוֹצֵר', 'אֲשֶׁר', 'מֵעוּדַי', 'גָּבַר', 'עָלַי', 'חַסְדּוֹ'] len: 11\n",
      "nikud: [['ֶּ', 'ַ', ''], ['ָּ', 'ָ', ''], ['ִ', 'ְ', 'ָּ', 'ֵ', 'ְ'], ['ַ', 'ּ', 'ֹ', 'ֵ', ''], ['ְ', 'ִ', 'ְׁ', 'ַּ', 'ַּ', ''], ['ַ', 'ּ', 'ֹ', 'ֵ', ''], ['ֲ', 'ֶׁ', ''], ['ֵ', '', 'ּ', 'ַ', ''], ['ָּ', 'ַ', ''], ['ָ', 'ַ', ''], ['ַ', 'ְ', 'ּ', 'ֹ']] len: 11\n",
      "Original word == word with nikud\n",
      "Original word == word with nikud\n",
      "Original word == word with nikud\n",
      "Original word == word with nikud\n",
      "Original word == word with nikud\n",
      "Original word == word with nikud\n",
      "Original word == word with nikud\n",
      "Original word == word with nikud\n",
      "Original word == word with nikud\n",
      "Original word == word with nikud\n",
      "Original word == word with nikud\n"
     ]
    }
   ],
   "source": [
    "# test the functions for adding and removing nikud\n",
    "text = read_txt_file(os.path.join(texts_path, 'afikeiyam1__nikud_meteg', 'afikeiyam1-002__nikud_meteg.txt'))\n",
    "# take just the first 100 characters\n",
    "text = text[:100]\n",
    "# remove extra spaces\n",
    "text = re.sub(' +', ' ', text)\n",
    "# trim text\n",
    "text = text.strip()\n",
    "# text_no_nikud = remove_nikud(text)\n",
    "# text_nikud = get_nikud(text)\n",
    "# text_with_nikud = add_nikud_to_text(text_no_nikud, text_nikud)\n",
    "\n",
    "sentence_words = text.split()\n",
    "print(f'sentence_words: {sentence_words} len: {len(sentence_words)}')\n",
    "# remove the nikud from the sentence\n",
    "sentence_without_nikud = remove_nikud(text)\n",
    "sentence_words_without_nikud = sentence_without_nikud.split()\n",
    "# get the nikud from the sentence\n",
    "nikud = list(map(get_nikud, sentence_words))\n",
    "print(f'nikud: {nikud} len: {len(nikud)}')\n",
    "for i in range(len(sentence_words)):\n",
    "    # print(f'Original word: {sentence_words[i]}')\n",
    "    # print(f'Nikud: {nikud[i]}')\n",
    "    # print(f'Word with nikud: {add_nikud(sentence_words_without_nikud[i], nikud[i])}')\n",
    "    print(f'Original word {\"==\" if sentence_words[i] == add_nikud(sentence_words_without_nikud[i], nikud[i]) else \"!=\"} word with nikud')\n",
    "\n",
    "# sentence_with_nikud = []\n",
    "# for i, word in enumerate(sentence_words):\n",
    "#     sentence_with_nikud.append(add_nikud_to_text(word, nikud[i]))\n",
    "# print(' '.join(sentence_with_nikud))\n",
    "\n",
    "# print(\"original text:\\n\", text)\n",
    "# print(\"text with added nikud:\\n\", text_with_nikud)\n",
    "# print(\"text without nikud:\\n\", text_no_nikud)\n",
    "# print(\"nikud array:\\n\", text_nikud)\n",
    "# print(\"original is equal to text with added nikud:\", text == text_with_nikud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a unified csv of all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "#dictionary of nikud to name\n",
    "nikud_dict = { 'ְ' : 'Sheva', 'ֱ' : 'Hataf Segol', 'ֲ' : 'Hataf Patah', 'ֳ' : 'Hataf Qamats', 'ִ' : 'Hiriq', 'ֵ' : 'Tseri', 'ֶ' : 'Segol', 'ַ' : 'Patah', 'ָ' : 'Qamats', 'ֹ' : 'Holam', 'ֻ' : 'Qubuts', 'ּ' : 'Dagesh', 'ֽ' : 'Siluk', '־' : 'Maqaf', 'ֿ' : 'Rafe', 'ׁ' : 'Shin Dot', 'ׂ' : 'Sin Dot', 'ׄ' : 'Upper Dot', 'ׅ' : 'Lower Dot', 'ׇ' : 'Point Meteg', 'װ' : 'Yiddish Double Vav', 'ױ' : 'Yiddish Vav Yod', 'ײ' : 'Yiddish Double Yod', '׳' : 'Geresh', '״' : 'Gershayim' }\n",
    "# make inverse dictionary\n",
    "nikud_dict_inv = {v: k for k, v in nikud_dict.items()}\n",
    "if 'full_data_without_puncts.csv' not in os.listdir('data'):\n",
    "    columns = ['text_with_nikud', 'text_without_nikud', 'nikud', 'author', 'file_name', 'sentence_num']\n",
    "    with open('data/full_data_without_puncts.csv', 'w', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(columns)\n",
    "        for author in tqdm(author_files_dict):\n",
    "            for file in author_files_dict[author]:\n",
    "                text = read_txt_file(os.path.join(texts_path, author, file))\n",
    "                sentences = re.split(r'\\n|\\.', text)\n",
    "                for i, sentence in enumerate(sentences):\n",
    "                    sentence = re.sub('\\'|\\\"|\\(|\\)', '', sentence)\n",
    "                    # Remove Dagesh, Shin Dot, Sin Dot using nikud_dict_inv\n",
    "                    sentence = re.sub(nikud_dict_inv['Dagesh'], '', sentence)\n",
    "                    sentence = re.sub(nikud_dict_inv['Shin Dot'], '', sentence)\n",
    "                    sentence = re.sub(nikud_dict_inv['Sin Dot'], '', sentence)\n",
    "                    # Keep only the nikud and hebrew letters and spaces\n",
    "                    sentence = re.sub(r'[^א-ת' + ''.join(nikud_dict.keys()) + '\\s]', '', sentence)\n",
    "                    # remove multiple spaces\n",
    "                    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "                    sentence = sentence.strip()\n",
    "                    if sentence == '':\n",
    "                        continue\n",
    "                    # split the sentence into words\n",
    "                    sentence_words = sentence.split()\n",
    "                    # remove the nikud from the sentence\n",
    "                    sentence_without_nikud = remove_nikud(sentence)\n",
    "                    # make sure first char is not in [\\u0591-\\u05C7]\n",
    "                    sentence_words = list(map(remove_first_char_if_nikud, sentence_words))\n",
    "                    sentence_words = [word for word in sentence_words if word != '']\n",
    "                    # get the nikud from the sentence\n",
    "                    nikud = list(map(get_nikud, sentence_words))\n",
    "                \n",
    "                    flag = True\n",
    "                    no_nikud_words = sentence_without_nikud.split()\n",
    "                    for j in range(len(no_nikud_words)):\n",
    "                        if sentence_words[j] != add_nikud(no_nikud_words[j], nikud[j]):\n",
    "                            test_word = sentence_words[j]\n",
    "                            flag = False\n",
    "                            print('No match')\n",
    "                                \n",
    "                    # add the sentence to the dataframe\n",
    "                    writer.writerow([sentence, sentence_without_nikud, nikud, author, file, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data from the csv and remove sentences with more than 100 chars (without spaces) and save as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #read only first 100000 rows of the CSV, we will use this for now\n",
    "# data_df = pd.read_csv('data/full_data_without_puncts.csv', nrows=200000, converters={'nikud': eval})\n",
    "# print(data_df.head())\n",
    "\n",
    "# # keep only rows s.t the length of the text (without spaces) is at most 100\n",
    "# data_df = data_df[data_df['text_without_nikud'].str.replace(' ', '').str.len() <= 100]\n",
    "\n",
    "# # save the dataframe to a json file\n",
    "# data_df.to_json('data/full_data_without_puncts.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### read the json short data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the json file\n",
    "data_df = pd.read_json('data/full_data_without_puncts.json', orient='records', lines=True, nrows=55000)\n",
    "print(data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE this block, only for debugging make the data smaller (randomly select 5000 rows)\n",
    "data_df = data_df.sample(n=5000, random_state=1)\n",
    "print(data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" block explanation\n",
    "# This block loops through each row in the data_df dataframe, reconstructs the original sentence with nikud, and compares it to the original sentence.\n",
    "# If the reconstructed sentence is not the same as the original text, it prints debugging information and checks character by character to find where the two strings differ.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Loop through each row in the data_df dataframe\n",
    "for i in range(len(data_df)):\n",
    "    \n",
    "    # Get the original text, processed text, and nikud for the current row\n",
    "    original_text = data_df.iloc[i]['text_with_nikud']\n",
    "    processed_text = data_df.iloc[i]['text_without_nikud'].split()\n",
    "    nikud = data_df.iloc[i]['nikud']\n",
    "    \n",
    "    # Initialize an empty string to hold the reconstructed sentence\n",
    "    sentence = ''\n",
    "    \n",
    "    # Loop through each word in the processed text and add the corresponding nikud\n",
    "    for j in range(len(processed_text)):\n",
    "        sentence += add_nikud(processed_text[j], nikud[j]) + ' '\n",
    "    \n",
    "    # Remove the last space from the sentence\n",
    "    sentence = sentence[:-1]\n",
    "    \n",
    "    # If the reconstructed sentence is not the same as the original text, print some debugging information\n",
    "    if original_text.strip(' ') != sentence:\n",
    "        print('Original text:\\n', original_text.strip(' '))\n",
    "        print('Original text length: ', len(original_text.strip(' ')))\n",
    "        print('Processed text:\\n', sentence)\n",
    "        print('Processed text length: ', len(sentence))\n",
    "        \n",
    "        # Check character by character to find where the two strings differ\n",
    "        for k in range(len(original_text.strip(' '))):\n",
    "            if original_text.strip(' ')[k] != sentence[k]:\n",
    "                print('Character: ', original_text.strip(' ')[k])\n",
    "                print('Character: ', sentence[k])\n",
    "                print('Index: ', k)\n",
    "                break\n",
    "        \n",
    "        print('-----------------------------------')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define dictionary label_to_id and id_to_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id = {'<no_nikud>': 0}\n",
    "id_to_label = {0: '<no_nikud>'}\n",
    "for label_list in tqdm(data_df['nikud']):\n",
    "    # flatten the list\n",
    "    label_list = [item for sublist in label_list for item in sublist]\n",
    "    for label in label_list:\n",
    "        if len(label) == 0:\n",
    "            label = \"<no_nikud>\"\n",
    "        if label not in label_to_id:\n",
    "            label_to_id[label] = len(label_to_id)\n",
    "            id_to_label[len(id_to_label)] = label\n",
    "\n",
    "print(label_to_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count labels for label_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of each label\n",
    "label_count = {}\n",
    "for label_list in tqdm(data_df['nikud']):\n",
    "    # Flatten the list\n",
    "    label_list = [item for sublist in label_list for item in sublist]\n",
    "    for label in label_list:\n",
    "        if len(label) == 0:\n",
    "            label = \"<no_nikud>\"\n",
    "        if label not in label_count:\n",
    "            label_count[label] = 0\n",
    "        label_count[label] += 1\n",
    "\n",
    "print(label_count)\n",
    "\n",
    "# Plot counts of each label (sorted)\n",
    "sorted_labels = sorted(label_count.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_labels)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Label Counts')\n",
    "plt.bar([x[0] for x in sorted_labels], [x[1] for x in sorted_labels])\n",
    "plt.show()\n",
    "\n",
    "# Define the label weights\n",
    "label_weights = {}\n",
    "for label in label_count:\n",
    "    if label == \"<no_nikud>\":\n",
    "        label_weights[label] = 0.33\n",
    "    else:\n",
    "        label_weights[label] = 1\n",
    "\n",
    "# Normalize the weights\n",
    "sum_weights = sum(label_weights.values())\n",
    "for label in label_weights:\n",
    "    label_weights[label] /= sum_weights\n",
    "\n",
    "def set_no_nikud_weight(weight=0.33):\n",
    "    label_weights = {}\n",
    "    for label in label_count:\n",
    "        if label == \"<no_nikud>\":\n",
    "            label_weights[label] = weight\n",
    "        else:\n",
    "            label_weights[label] = 1\n",
    "    sum_weights = sum(label_weights.values())\n",
    "    for label in label_weights:\n",
    "        label_weights[label] /= sum_weights\n",
    "    return label_weights\n",
    "        \n",
    "\n",
    "print(label_weights)\n",
    "\n",
    "for label in label_weights.keys():\n",
    "    if label == \"<no_nikud>\":\n",
    "        print(f'f label name = {label}')\n",
    "    elif len(label) == 1:\n",
    "        print(f'label name = {nikud_dict[label]}')\n",
    "    else:\n",
    "        label_name = \"\"\n",
    "        for char in label:\n",
    "            label_name += \" \" + nikud_dict[char]\n",
    "        print(f'label name = {label_name}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA - DataAnalysis on short_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "class DataAnalysis:\n",
    "    def __init__(self, data_df):\n",
    "        self.data_df = data_df.copy()\n",
    "        \n",
    "    def author_distribution(self):\n",
    "        # Distribution of authors\n",
    "        author_distribution = self.data_df['author'].value_counts()\n",
    "\n",
    "        # Bar plot of the distribution of the top 20 authors\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        author_distribution[:20].plot(kind='bar')\n",
    "        plt.xlabel('Author')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Top 20 Authors')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "        \n",
    "    def file_distribution(self):\n",
    "        # Distribution of files\n",
    "        file_distribution = self.data_df['file_name'].value_counts()\n",
    "\n",
    "        # Bar plot of the distribution of the top 20 files\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        file_distribution[:20].plot(kind='bar')\n",
    "        plt.xlabel('File')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Top 20 Files')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "        \n",
    "    def sentence_length_distribution(self):\n",
    "        # Create a new column for sentence length\n",
    "        self.data_df['sentence_length'] = self.data_df['text_without_nikud'].str.split().str.len()\n",
    "\n",
    "        # Get the distribution of sentence lengths\n",
    "        sentence_length_distribution = self.data_df['sentence_length'].value_counts().sort_index()\n",
    "\n",
    "        # Histogram of the distribution of sentence lengths\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(self.data_df['sentence_length'], bins=50, alpha=0.5)\n",
    "        plt.xlabel('Sentence Length')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distribution of Sentence Lengths')\n",
    "        plt.show()\n",
    "        \n",
    "    def common_words(self):\n",
    "        # Tokenize the sentences\n",
    "        tokenized_sentences = self.data_df['text_without_nikud'].str.split()\n",
    "\n",
    "        # Flatten the list of tokens and count the frequency of each token\n",
    "        word_counts = Counter(chain.from_iterable(tokenized_sentences))\n",
    "\n",
    "        # Get the 20 most common words\n",
    "        common_words = word_counts.most_common(20)\n",
    "\n",
    "        # Bar plot of the frequency of the 20 most common words\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        words, counts = zip(*common_words)\n",
    "        plt.bar(words, counts)\n",
    "        plt.xlabel('Word')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('20 Most Common Words')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "        \n",
    "    def common_nikud(self):\n",
    "        # Flatten the list of Nikud and count the frequency of each Nikud\n",
    "        nikud_counts = Counter(chain.from_iterable(chain.from_iterable(self.data_df['nikud'])))\n",
    "\n",
    "        # Get the 20 most common Nikud\n",
    "        common_nikud = nikud_counts.most_common(20)\n",
    "\n",
    "        # Bar plot of the frequency of the 20 most common Nikud\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        nikud, counts = zip(*common_nikud)\n",
    "        plt.bar(nikud, counts)\n",
    "        plt.xlabel('Nikud')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('20 Most Common Nikud')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "\n",
    "# instantiate the class and call the methods\n",
    "# data_analysis = DataAnalysis(data_df)\n",
    "# data_analysis.author_distribution()\n",
    "# data_analysis.file_distribution()\n",
    "# data_analysis.sentence_length_distribution()\n",
    "# data_analysis.common_words()\n",
    "# data_analysis.common_nikud()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download tokenizer and model\n",
    "(alephbert-base, with vocab of words with len <= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'C:\\\\Users\\\\baruc\\\\PycharmProjects\\\\pythonProject\\\\Punctuation_Restoration\\\\AlephBERT-main\\\\AlephBERT-main\\\\models\\\\alephbert-base'\n",
    "alephbert_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "alephbert_model = AutoModelForMaskedLM.from_pretrained(\"onlplab/alephbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the tokenization and detokenization\n",
    "test = \"בדיקה של הדבר הזה\"\n",
    "tokenized = alephbert_tokenizer.tokenize(test)\n",
    "encoded = alephbert_tokenizer.encode(test)\n",
    "decoded = alephbert_tokenizer.decode(encoded)\n",
    "print(test)\n",
    "print(\"tokenized: \", tokenized)\n",
    "print(\"encoded: \", encoded)\n",
    "print(\"decoded: \", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.iloc[0]\n",
    "text = data_df.iloc[0]['text_without_nikud']\n",
    "nikud = data_df.iloc[0]['nikud']\n",
    "for row in data_df.iterrows():\n",
    "    text = row[1]['text_without_nikud']\n",
    "    text_with_nikud = row[1]['text_with_nikud']\n",
    "    nikud = row[1]['nikud']\n",
    "    text_words = text.split()\n",
    "    sentence = []\n",
    "    for i in range(len(nikud)):\n",
    "        sentence.append(add_nikud(text_words[i], nikud[i]))\n",
    "    sentence = ' '.join(sentence)\n",
    "    if sentence != text_with_nikud:\n",
    "        print(sentence)\n",
    "        print(text_with_nikud)\n",
    "        print('------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create DataSet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch dataset class for punctuation restoration (returns input(text) and target(nikud))\n",
    "\n",
    "class PunctuationRestorationDataset(Dataset):\n",
    "    def __init__(self, data_df, tokenizer, label_to_id, max_len):\n",
    "        self.data = data_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label_to_id = label_to_id \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index): # TODO: need to make sure not to look at fraction of words\n",
    "        text = self.data.iloc[index]['text_without_nikud']\n",
    "        nikud = self.data.iloc[index]['nikud']  # list of lists of nikud\n",
    "\n",
    "        # flatten nikud list\n",
    "        nikud = [item for sublist in nikud for item in sublist]\n",
    "        # replace empty strings with <no_nikud> token\n",
    "        nikud = [label if label != \"\" else \"<no_nikud>\" for label in nikud]\n",
    "        # replace labels with ids\n",
    "        nikud = [self.label_to_id[label] for label in nikud]\n",
    "\n",
    "        # check if nikud length is the same as text without spaces length\n",
    "        if len(text.replace(\" \", \"\")) != len(nikud):\n",
    "            print(\"text without spaces length: \", len(text.replace(\" \", \"\")))\n",
    "            print(\"nikud length: \", len(nikud))\n",
    "        \n",
    "        # get word indices after tokenization\n",
    "        word_indices = get_words_indices_from_text(text)\n",
    "        # pad word_indices to be the same length as input_ids\n",
    "        word_indices = word_indices + [(-1, -1)] * (self.max_len - len(word_indices))\n",
    "        # convert word_indices to tensor\n",
    "        word_indices = torch.tensor(word_indices, dtype=torch.long)\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # pad nikud to be the same length as input_ids\n",
    "        nikud = nikud + [len(label_to_id)] * (encoding['input_ids'].shape[1] - len(nikud))\n",
    "        # convert to tensor\n",
    "        nikud = torch.tensor(nikud, dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'nikud': nikud,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'word_indices': word_indices\n",
    "        }\n",
    "\n",
    "\n",
    "# create pytorch dataset class for punctuation restoration (returns input(text) and target(nikud))\n",
    "dataset = PunctuationRestorationDataset(data_df, alephbert_tokenizer, label_to_id, 100)\n",
    "print(\"dataset length: \", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #iterate over dataset\n",
    "for i in range(len(dataset)):\n",
    "    sample = dataset[i]\n",
    "    Original = data_df.iloc[i]['text_with_nikud']\n",
    "    text_without_nikud = sample['text']\n",
    "    nikud = sample['nikud']\n",
    "    input_ids = sample['input_ids']\n",
    "    attention_mask = sample['attention_mask']\n",
    "    word_indices = sample['word_indices']\n",
    "    \n",
    "    # convert nikud_id to nikud_label\n",
    "    nikud_after_id_to_label = [id_to_label[id] if id < len(id_to_label) else \"<no_nikud>\" for id in nikud.tolist()]\n",
    "    # add nikud to text without nikud\n",
    "    text_words = text_without_nikud.split()\n",
    "    text_words_with_nikud = []\n",
    "    for word_index, word in enumerate(text_words):\n",
    "        text_words_with_nikud.append(add_nikud(word, nikud_after_id_to_label[word_indices[word_index][0]:word_indices[word_index][1]+1]))\n",
    "    text_with_nikud = \" \".join(text_words_with_nikud)\n",
    "    text_with_nikud = text_with_nikud.replace(\"<no_nikud>\", \"\")\n",
    "    if text_with_nikud != Original:\n",
    "        print(\"text_with_nikud: \", text_with_nikud)\n",
    "        print(\"Original: \", Original)\n",
    "    print(\"word_indices: \", word_indices)\n",
    "    # convert input_ids to tokens\n",
    "    tokens = alephbert_tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    if i > 5:\n",
    "        break\n",
    "    \n",
    "    # # Check that nikud alligns with tokens\n",
    "    # nikud = nikud.tolist()\n",
    "    # # zip tokens and nikud\n",
    "    # tokens_with_nikud = list(zip(tokens, nikud))\n",
    "    # # print(\"tokens_with_nikud: \", tokens_with_nikud)\n",
    "    # for token, label in tokens_with_nikud:\n",
    "    #     if token == \"[PAD]\" and label != len(id_to_label) or token != \"[PAD]\" and label == len(id_to_label):\n",
    "    #         print(\"token: \", token)\n",
    "    #         print(\"label: \", label)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split to train,val and test datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset to train, val and test\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# create dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# performe EDAs on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA on train, val and test dataset\n",
    "print(\"train dataset size: \", len(train_dataset))\n",
    "print(\"val dataset size: \", len(val_dataset))\n",
    "print(\"test dataset size: \", len(test_dataset))\n",
    "\n",
    "# train dataset\n",
    "print(\"train dataset type: \", type(train_dataset))\n",
    "print(\"train dataset[0] type: \", type(train_dataset[0]))\n",
    "print(\"train dataset[0] keys: \", train_dataset[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the two models (one with look-ahead, one without)\n",
    "\n",
    "### train the two models\n",
    "\n",
    "### Evaluate the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full-Sentence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a charachter level transformer model with the following architecture:\n",
    "# 1. Embedding layer\n",
    "# 2. Transformer layer\n",
    "# 3. Fully connected layer\n",
    "\n",
    "embedding_dim = 512\n",
    "\n",
    "class FullSentenceModel(nn.Module):\n",
    "    \"\"\" Transformer model for Nikud restoration. \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim=512, nhead=8, num_layers=6, output_dim=len(label_to_id), dropout=0.1):\n",
    "        super(FullSentenceModel, self).__init__()\n",
    "        \n",
    "        # Non-pretrained embedding layer\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        \n",
    "        self.transformer = nn.Transformer(d_model=embedding_dim, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers, dropout=dropout, activation='relu')\n",
    "\n",
    "        # try with just encoder\n",
    "        #self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead, dropout=dropout), num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # Pass input through embedding layer\n",
    "        characters_embeddings = self.embedding(sentence)\n",
    "        \n",
    "        # Pass through transformer\n",
    "        characters_after_transformer_layers = self.transformer(characters_embeddings, characters_embeddings)\n",
    "        #characters_after_transformer_layers = self.transformer(characters_embeddings)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        predictions = self.fc(characters_after_transformer_layers)\n",
    "        \n",
    "        return predictions\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adding positional encoding to the FullSentenceModel\n",
    "# import math \n",
    "# class FullSentenceModel(nn.Module):\n",
    "#     \"\"\" Transformer model for Nikud restoration with positional encoding. \"\"\"\n",
    "    \n",
    "#     def __init__(self, vocab_size, embedding_dim=512, nhead=8, num_layers=6, output_dim=len(label_to_id), dropout=0.1, max_len=500):\n",
    "#         super(FullSentenceModel, self).__init__()\n",
    "        \n",
    "#         self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        \n",
    "#         self.transformer = nn.Transformer(d_model=embedding_dim, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers, dropout=dropout, activation='relu')\n",
    "        \n",
    "#         self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        \n",
    "#         # Positional encoding addition\n",
    "#         self.pos_encoder = nn.Embedding(max_len, embedding_dim)\n",
    "#         self.max_len = max_len\n",
    "#         position = torch.arange(0, max_len, dtype=torch.long).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
    "#         self.pos_encoder.weight.data[:, 0::2] = torch.sin(position * div_term)\n",
    "#         self.pos_encoder.weight.data[:, 1::2] = torch.cos(position * div_term)\n",
    "#         self.pos_encoder.weight.detach_()\n",
    "#         self.pos_encoder.weight.requires_grad = True\n",
    "\n",
    "#     def forward(self, sentence):\n",
    "#         # Pass input through embedding layer\n",
    "#         characters_embeddings = self.embedding(sentence)\n",
    "        \n",
    "#         # Add positional encodings\n",
    "#         seq_len = sentence.size(1)\n",
    "#         pos = torch.arange(0, seq_len, dtype=torch.long, device=sentence.device).unsqueeze(0)\n",
    "#         characters_embeddings = characters_embeddings + self.pos_encoder(pos)\n",
    "        \n",
    "#         # Pass through transformer\n",
    "#         characters_after_transformer_layers = self.transformer(characters_embeddings, characters_embeddings)\n",
    "        \n",
    "#         # Pass through fully connected layer\n",
    "#         predictions = self.fc(characters_after_transformer_layers)\n",
    "        \n",
    "#         return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "vocab_size = len(alephbert_tokenizer.vocab)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=len(label_to_id)).to(device)\n",
    "weights = torch.tensor(list(label_weights.values())).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=len(label_to_id), weight=weights).to(device)\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "# define the training loop\n",
    "def train_full_model(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['nikud'].to(device)\n",
    "        predictions = model.forward(input_ids)#, labels)\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        # # convert labels of no_nikud to ignore_index (padding index)\n",
    "        # labels[labels == label_to_id['<no_nikud>']] = len(label_to_id)\n",
    "\n",
    "        loss = criterion(predictions, labels)\n",
    "        train_loss_history.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# define the evaluation loop\n",
    "def evaluate_full_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['nikud'].to(device)\n",
    "            predictions = model.forward(input_ids)\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            labels = labels.view(-1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_loss_history.append(loss.item())\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# define the training and evaluation loop\n",
    "def train_and_evaluate_full_model(model, train_dataloader, val_dataloader, optimizer, criterion, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_full_model(model, train_dataloader, optimizer, criterion)\n",
    "        scheduler.step()\n",
    "        val_loss = evaluate_full_model(model, val_dataloader, criterion)\n",
    "        print(f'Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        # plot train and val loss history\n",
    "        plt.plot(train_loss_history, label='train')\n",
    "        plt.plot(val_loss_history, label='val')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def hyperparameter_grid_search():\n",
    "    # train the model\n",
    "    nhead = [2, 4, 8]\n",
    "    nlayers = [2, 4, 6]\n",
    "    lr = [0.0001, 0.00001]\n",
    "    for n in nhead:\n",
    "        for l in nlayers:\n",
    "            for r in lr:\n",
    "                model = FullSentenceModel(vocab_size, nhead=n, num_layers=l).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=r)\n",
    "                print(f'nhead: {n}, nlayers: {l}, lr: {r}')\n",
    "                train_and_evaluate_full_model(model, train_loader, val_loader, optimizer, criterion, epochs=5)\n",
    "\n",
    "# Imprortant reminder: Optimizer should be defined for each model separately\n",
    "# model = FullSentenceModel(vocab_size, nhead=8, num_layers=6, embedding_dim=512).to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# # Define AdamW optimizer\n",
    "# # optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "# scheduler = StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "# print number of trainable parameters\n",
    "# print(f'Number of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n",
    "\n",
    "label_weights_list = [1]#, 0.5, 0.33, 0.25]\n",
    "for weight in label_weights_list:\n",
    "    train_loss_history = []\n",
    "    print(f'Starting weight: {weight}')\n",
    "    full_sentence_model = FullSentenceModel(vocab_size, nhead=8, num_layers=6, embedding_dim=512).to(device)\n",
    "    print(f'Number of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n",
    "    optimizer = torch.optim.Adam(full_sentence_model.parameters(), lr=0.0001)\n",
    "    # Define AdamW optimizer\n",
    "    # optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "    label_weights = set_no_nikud_weight(weight)\n",
    "    weights = torch.tensor(list(label_weights.values())).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=len(label_to_id), weight=weights).to(device)\n",
    "    train_and_evaluate_full_model(full_sentence_model, train_loader, val_loader, optimizer, criterion, epochs=2)\n",
    "    print(f'Finished weight: {weight}')\n",
    "    print('-----------------------------------')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_full_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Predict the labels of the test set using the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    actual_labels = []\n",
    "    predictions = []\n",
    "    all_words = []\n",
    "    for batch in tqdm(test_loader):\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            word_indices = batch['word_indices'].to(device)\n",
    "            labels = batch['nikud'].to(device)\n",
    "            \n",
    "            # get the model's predictions\n",
    "            batch_predictions = model.forward(input_ids)\n",
    "            batch_predictions = batch_predictions.view(-1, batch_predictions.shape[-1])\n",
    "            labels = labels.view(-1)\n",
    "            word_indices = word_indices.view(-1, 2)\n",
    "            \n",
    "            \n",
    "            # remove the padding tokens from the predictions\n",
    "            batch_predictions = batch_predictions[attention_mask.view(-1) == 1]\n",
    "            labels = labels[attention_mask.view(-1) == 1]\n",
    "            word_indices = word_indices[word_indices[:, 0] != -1]\n",
    "            \n",
    "            # get the top 2 predictions for each token\n",
    "            batch_predictions = torch.topk(batch_predictions, k=2, dim=1).indices\n",
    "            first_prediction = batch_predictions[:, 0]\n",
    "            second_prediction = batch_predictions[:, 1]\n",
    "            \n",
    "            predictions.extend(first_prediction)\n",
    "            actual_labels.extend(labels)\n",
    "            all_words.extend(word_indices)\n",
    "    all_words = [word.tolist() for word in all_words]\n",
    "    all_words = list(map(lambda x: x[1]-x[0]+1, all_words))\n",
    "    return actual_labels, predictions, all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train and val loss\n",
    "plt.plot(train_loss_history, label='train')\n",
    "plt.plot(val_loss_history, label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    # predict on the first batch of the test set\n",
    "    full_sentence_model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['nikud'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        word_indices = batch['word_indices'].to(device)\n",
    "        \n",
    "        # print some information about the batch\n",
    "        # print(\"number of zeros in input_ids: \", torch.sum(input_ids == 0))\n",
    "        # print(\"number of 0 in attention_mask: \", torch.sum(attention_mask == 0))\n",
    "        # print(\"number of paddings in labels: \", torch.sum(labels == len(id_to_label)))\n",
    "\n",
    "        # convert the input_ids to tokens\n",
    "        tokens = alephbert_tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "        # enumerate the tokens and their ids and save them in a dictionary\n",
    "        token_ids = {i: token for i, token in enumerate(zip(input_ids[0].tolist(), tokens))}\n",
    "        # save token_ids in a dataframe\n",
    "        token_ids_df = pd.DataFrame.from_dict(token_ids, orient='index', columns=['id', 'token'])\n",
    "        # add the labels to the dataframe\n",
    "        token_ids_df['label'] = labels[0].tolist()\n",
    "        \n",
    "        # make sure ['PAD'] recieved the correct label\n",
    "        if not (token_ids_df[token_ids_df['token'] == '[PAD]']['label'] == len(id_to_label)).all():\n",
    "            print(\"ERROR\")\n",
    "        \n",
    "        # get the model's predictions and calculate the loss\n",
    "        predictions = full_sentence_model.forward(input_ids)\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        labels = labels.view(-1)\n",
    "        word_indices = word_indices.view(-1, 2)\n",
    "\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # print some information about the predictions\n",
    "        # print(\"input_ids: \", input_ids)\n",
    "        # print(\"labels: \", labels)\n",
    "        # print(\"attention_mask: \", attention_mask)\n",
    "        # print(\"predictions: \", predictions)\n",
    "\n",
    "        print(f'Predictions shape before removing padding: {predictions.shape}')\n",
    "        print(f'Labels shape before removing padding: {labels.shape}')\n",
    "        print(f'Word indices shape before removing padding: {word_indices.shape}')\n",
    "\n",
    "        print(f'Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # remove the padding tokens from the predictions and labels\n",
    "        predictions = predictions[attention_mask.view(-1) == 1]\n",
    "        \n",
    "        labels = labels[attention_mask.view(-1) == 1]\n",
    "        word_indices = word_indices[word_indices[:, 0] != -1]\n",
    "        print(f'Predictions shape after removing padding: {predictions.shape}')\n",
    "        print(f'Labels shape after removing padding: {labels.shape}')\n",
    "        print(f'Word indices shape after removing padding: {word_indices.shape}')\n",
    "        \n",
    "        \n",
    "        # get the top 2 predictions for each token\n",
    "        predictions = torch.topk(predictions, k=2, dim=1).indices\n",
    "        first_prediction = predictions[:, 0]\n",
    "        second_prediction = predictions[:, 1]\n",
    "\n",
    "        # calculate the accuracy of the model's predictions\n",
    "        correct_sum = 0\n",
    "        overall_sum = 0\n",
    "        for i in range(len(first_prediction)):\n",
    "            if first_prediction[i] != 0:\n",
    "                overall_sum += 1\n",
    "                if first_prediction[i] == labels[i]:\n",
    "                    correct_sum += 1\n",
    "        accuracy = correct_sum / overall_sum\n",
    "        print(f'Accuracy of actual nikud: {accuracy:.4f}')\n",
    "        \n",
    "        overall_sum = torch.sum(first_prediction == labels)\n",
    "        \n",
    "        # calculate the overall accuracy of the model's predictions\n",
    "        accuracy = overall_sum / len(labels)\n",
    "        print(f'Accuracy: {accuracy.item():.4f}')\n",
    "        \n",
    "        # print the model's predictions and the actual labels for the first 50 tokens\n",
    "        # print(\"Predictions\\tLabels\\t\\tFirst\\t\\tSecond\\t\\tChar\")\n",
    "        # for i in range(100):\n",
    "        #     if token_ids_df.iloc[i][\"token\"] == '[PAD]':\n",
    "        #         break\n",
    "        #     print(f'{predictions[i].tolist()}\\t\\t{labels[i]}\\t\\t{predictions[i].tolist()[0] == labels[i]}\\t\\t{predictions[i].tolist()[1] == labels[i]}\\t\\t{token_ids_df.iloc[i][\"token\"]}')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading-Direction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadingDirectionModel(nn.Module):\n",
    "    def __init__(self, char_vocab_size, num_labels, char_embedding_dim=32, word_hidden_dim=16, char_hidden_dim=512, num_layers=4):\n",
    "        super(ReadingDirectionModel, self).__init__()\n",
    "        \n",
    "        self.word_hidden_dim = word_hidden_dim\n",
    "        \n",
    "        # Character Embedding Layer\n",
    "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim)\n",
    "        \n",
    "        # Word-Level BiLSTM\n",
    "        self.word_bilstm = nn.LSTM(char_embedding_dim, word_hidden_dim, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Character-Level LSTM\n",
    "        self.char_lstm = nn.LSTM(input_size=char_embedding_dim + 2 * word_hidden_dim, hidden_size=char_hidden_dim, batch_first=True, num_layers=num_layers)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(char_hidden_dim, num_labels)\n",
    "        \n",
    "        # Dropout Layer\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, char_sequence, word_boundaries):\n",
    "        # Apply Character Embedding\n",
    "        char_embedded = self.char_embedding(char_sequence)\n",
    "        \n",
    "        # Process each word using the Word-Level BiLSTM\n",
    "        word_embeddings_list = []\n",
    "        for i in range(len(char_sequence)):\n",
    "            word_embedding_for_sample = []\n",
    "            for boundaries in word_boundaries[i]:\n",
    "                start, end = boundaries\n",
    "                if start == -1:\n",
    "                    break\n",
    "                word_seq = char_embedded[i, start:end+1, :]\n",
    "                word_seq_output, _ = self.word_bilstm(word_seq.unsqueeze(0))\n",
    "                word_embedding = torch.cat((word_seq_output[0, -1, :self.word_hidden_dim], \n",
    "                                            word_seq_output[0, 0, self.word_hidden_dim:]), dim=0)\n",
    "                word_embedding_for_sample.append(word_embedding)\n",
    "            word_embeddings_list.append(torch.stack(word_embedding_for_sample))\n",
    "        \n",
    "        # Pad the word embeddings to the same length as the input sequence\n",
    "        max_length = char_embedded.size(1)\n",
    "        padded_word_embeddings_list = []\n",
    "        for embed in word_embeddings_list:\n",
    "            padding_length = max_length - embed.size(0)\n",
    "            padding = torch.zeros(padding_length, embed.size(1)).to(embed.device)\n",
    "            padded_embed = torch.cat([embed, padding], dim=0)\n",
    "            padded_word_embeddings_list.append(padded_embed)\n",
    "        word_embeddings_tensor = torch.stack(padded_word_embeddings_list, dim=0)\n",
    "        \n",
    "        # Concatenate the character embeddings with the word embeddings\n",
    "        char_word_embedded = torch.cat([char_embedded, word_embeddings_tensor], dim=2)\n",
    "        \n",
    "        # Apply Character-Level LSTM\n",
    "        char_lstm_output, _ = self.char_lstm(char_word_embedded)\n",
    "        \n",
    "        # Apply Dropout\n",
    "        char_lstm_output = self.dropout(char_lstm_output)\n",
    "        \n",
    "        # Apply Fully Connected Layer\n",
    "        predictions = self.fc(char_lstm_output)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_directional_model(model, train_loader, optimizer, criterion, train_directional_model_loss_history):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_overall = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        word_indices = batch['word_indices'].to(device)\n",
    "        labels = batch['nikud'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model.forward(input_ids, word_indices)\n",
    "        loss = criterion(predictions.view(-1, predictions.shape[-1]), labels.view(-1))\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_directional_model_loss_history.append(loss.item())\n",
    "        \n",
    "        attention_mask = batch['attention_mask']\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        labels = labels.view(-1)\n",
    "        predictions = predictions[attention_mask.view(-1) == 1]\n",
    "        labels = labels[attention_mask.view(-1) == 1]\n",
    "        predictions = torch.argmax(predictions, dim=1)\n",
    "\n",
    "        # calculate accuracy\n",
    "        pre = predictions.cpu().numpy()\n",
    "        lab = labels.cpu().numpy()\n",
    "        total_correct += np.sum(pre == lab)\n",
    "        total_overall += len(pre)\n",
    "    accuracy = total_correct / total_overall\n",
    "    return total_loss / len(train_loader), accuracy\n",
    "\n",
    "# Define evaluate method\n",
    "def evaluate_directional_model(model, eval_loader, criterion, val_directional_model_loss_history):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_overall = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            word_indices = batch['word_indices'].to(device)\n",
    "            labels = batch['nikud'].to(device)\n",
    "            predictions = model.forward(input_ids, word_indices)\n",
    "            loss = criterion(predictions.view(-1, predictions.shape[-1]), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            # total_correct += (predictions.argmax(2) == labels).sum().item()\n",
    "            val_directional_model_loss_history.append(loss.item())\n",
    "\n",
    "            attention_mask = batch['attention_mask']\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            labels = labels.view(-1)\n",
    "            predictions = predictions[attention_mask.view(-1) == 1]\n",
    "            labels = labels[attention_mask.view(-1) == 1]\n",
    "            predictions = torch.argmax(predictions, dim=1)\n",
    "            # calculate accuracy\n",
    "            pre = predictions.cpu().numpy()\n",
    "            lab = labels.cpu().numpy()\n",
    "            total_correct += np.sum(pre == lab)\n",
    "            total_overall += len(pre)\n",
    "    accuracy = total_correct / total_overall\n",
    "    return total_loss / len(eval_loader), accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predict method\n",
    "def predict_directional_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Predict the labels of the test set using the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    actual_labels = []\n",
    "    predictions = []\n",
    "    all_words = []\n",
    "    for batch in tqdm(test_loader):\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            word_indices = batch['word_indices'].to(device)\n",
    "            labels = batch['nikud'].to(device)\n",
    "            \n",
    "            # get the model's predictions\n",
    "            batch_predictions = model.forward(input_ids, word_indices)\n",
    "            batch_predictions = batch_predictions.view(-1, batch_predictions.shape[-1])\n",
    "            labels = labels.view(-1)\n",
    "            word_indices = word_indices.view(-1, 2)\n",
    "            \n",
    "            # remove the padding tokens from the predictions\n",
    "            batch_predictions = batch_predictions[attention_mask.view(-1) == 1]\n",
    "            labels = labels[attention_mask.view(-1) == 1]\n",
    "            word_indices = word_indices[word_indices[:, 0] != -1]\n",
    "            \n",
    "            # get the top 2 predictions for each token\n",
    "            batch_predictions = torch.topk(batch_predictions, k=2, dim=1).indices\n",
    "            first_prediction = batch_predictions[:, 0]\n",
    "            second_prediction = batch_predictions[:, 1]\n",
    "            \n",
    "            predictions.extend(first_prediction)\n",
    "            actual_labels.extend(labels)\n",
    "            all_words.extend(word_indices)\n",
    "    all_words = [word.tolist() for word in all_words]\n",
    "    all_words = list(map(lambda x: x[1]-x[0]+1, all_words))\n",
    "    return actual_labels, predictions, all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_character_accuracy(actual_labels, predictions):\n",
    "    \"\"\"\n",
    "    Compute the character-level accuracy.\n",
    "    \"\"\"\n",
    "    correct = sum([1 for p, a in zip(predictions, actual_labels) if p == a])\n",
    "    total = len(predictions)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def compute_word_accuracy(actual_labels, predictions, word_lengths):\n",
    "    \"\"\"\n",
    "    Compute the word-level accuracy.\n",
    "    \"\"\"\n",
    "    current_index = 0\n",
    "    correct_words = 0\n",
    "    for word_length in word_lengths:\n",
    "        word = predictions[current_index : current_index + word_length]\n",
    "        word_labels = actual_labels[current_index : current_index + word_length]\n",
    "        comparison = all([p == a for p, a in zip(word, word_labels)])\n",
    "        if comparison:\n",
    "            correct_words += 1\n",
    "        current_index += word_length\n",
    "\n",
    "    return correct_words / len(word_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "char_vocab_size = alephbert_tokenizer.vocab_size\n",
    "char_embedding_dim = 32\n",
    "word_hidden_dim = 16\n",
    "char_hidden_dim = 512\n",
    "num_labels = len(label_to_id)\n",
    "\n",
    "# Model, Criterion, Optimizer\n",
    "reading_direction_model = ReadingDirectionModel(char_vocab_size, num_labels, char_embedding_dim=char_embedding_dim, word_hidden_dim=word_hidden_dim, char_hidden_dim=char_hidden_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=len(label_to_id)).to(device)\n",
    "optimizer = optim.Adam(reading_direction_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "train_directional_model_loss_history = []\n",
    "val_directional_model_loss_history = []\n",
    "# check if model is already trained\n",
    "if os.path.exists('directional_model_10_epoch.pt'): # TODO: Delete (and False) to load model\n",
    "    reading_direction_model.load_state_dict(torch.load('directional_model_10_epoch.pt'))\n",
    "    print('Model loaded successfully!')\n",
    "    with open('directional_model_10_epoch_loss_history.pkl', 'rb') as f:\n",
    "        train_directional_model_loss_history, val_directional_model_loss_history = pickle.load(f)\n",
    "    print('Model loss history loaded successfully!')\n",
    "else:\n",
    "    for epoch in range(10):\n",
    "        train_loss, train_accuracy = train_directional_model(reading_direction_model, train_loader, optimizer, criterion, train_directional_model_loss_history)\n",
    "        eval_loss, eval_accuracy = evaluate_directional_model(reading_direction_model, val_loader, criterion, val_directional_model_loss_history)\n",
    "        print(f'Epoch: {epoch+1}')\n",
    "        print(f'Train Loss: {train_loss:.3f} | Train Accuracy: {train_accuracy*100:.2f}%')\n",
    "        print(f'Eval Loss: {eval_loss:.3f} | Eval Accuracy: {eval_accuracy*100:.2f}%')\n",
    "        print('-'*10)\n",
    "        # plot train and eval loss\n",
    "        plt.plot(train_directional_model_loss_history, label='train')\n",
    "        plt.plot(val_directional_model_loss_history, label='eval')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # save directional_model_10_epoch\n",
    "    torch.save(reading_direction_model.state_dict(), 'directional_model_10_epoch.pt')\n",
    "    print('Model saved successfully!')\n",
    "    # save directional_model_10_epoch_loss_history\n",
    "    with open('directional_model_10_epoch_loss_history.pkl', 'wb') as f:\n",
    "        pickle.dump([train_directional_model_loss_history, val_directional_model_loss_history], f)\n",
    "    print('Model loss history saved successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(actuals, predictions, labels):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix for the model's predictions.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(actuals, predictions, labels=labels)\n",
    "    # cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    epsilon = 1e-8\n",
    "    cm = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + epsilon)\n",
    "    cm[np.isnan(cm)] = 0\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=labels, yticklabels=labels, fmt='.2f')\n",
    "    # sns.heathamap\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.show()\n",
    "\n",
    "def convert_labels_to_names(actuals, predictions, label_to_id):\n",
    "    \"\"\"\n",
    "    Convert the labels to their names.\n",
    "    \"\"\"\n",
    "    nikud_dict = { 'ְ' : 'Sheva', 'ֱ' : 'Hataf Segol', 'ֲ' : 'Hataf Patah', 'ֳ' : 'Hataf Qamats', 'ִ' : 'Hiriq', 'ֵ' : 'Tseri', 'ֶ' : 'Segol', 'ַ' : 'Patah', 'ָ' : 'Qamats', 'ֹ' : 'Holam', 'ֻ' : 'Qubuts', 'ּ' : 'Dagesh', 'ֽ' : 'Siluk', '־' : 'Maqaf', 'ֿ' : 'Rafe', 'ׁ' : 'Shin Dot', 'ׂ' : 'Sin Dot', 'ׄ' : 'Upper Dot', 'ׅ' : 'Lower Dot', 'ׇ' : 'Point Meteg', 'װ' : 'Yiddish Double Vav', 'ױ' : 'Yiddish Vav Yod', 'ײ' : 'Yiddish Double Yod', '׳' : 'Geresh', '״' : 'Gershayim'}\n",
    "    labels = list(label_to_id.keys())\n",
    "    label_names = {'<no_nikud>': 'No Nikud'}\n",
    "    for label in labels:\n",
    "        if label == '<no_nikud>':\n",
    "            continue\n",
    "        label_name = []\n",
    "        for char in label:\n",
    "            label_name.append(nikud_dict[char])\n",
    "        label_names[label] = ' '.join(label_name)\n",
    "    actuals = [label_names[label] for label in actuals]\n",
    "    predictions = [label_names[label] for label in predictions]\n",
    "    return actuals, predictions, label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Full Sentence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for the test set\n",
    "actuals, predictions, word_lengths = predict_full_model(full_sentence_model, test_loader)\n",
    "# convert the actual labels and the predictions to lists\n",
    "actuals = [id_to_label[label.item()] for label in actuals]\n",
    "predictions = [id_to_label[label.item()] for label in predictions]\n",
    "# predictions, actual_labels, all_words = predict_full_model(model, test_loader)\n",
    "char_accuracy = compute_character_accuracy(actuals, predictions)\n",
    "# word_boundaries = [batch[\"word_indices\"] for batch in test_loader]  # Assuming this is how word boundaries are stored.\n",
    "word_accuracy = compute_word_accuracy(actuals, predictions, word_lengths)\n",
    "\n",
    "print(f\"Character-level accuracy: {char_accuracy:.2%}\")\n",
    "print(f\"Word-level accuracy: {word_accuracy:.2%}\")\n",
    "# convert the labels to names\n",
    "actuals, predictions, label_names = convert_labels_to_names(actuals, predictions, label_to_id)\n",
    "plot_confusion_matrix(actuals, predictions, list(label_names.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Reading Direction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for the test set\n",
    "actuals, predictions, word_lengths = predict_directional_model(reading_direction_model, test_loader)\n",
    "# convert the actual labels and the predictions to lists\n",
    "actuals = [id_to_label[label.item()] for label in actuals]\n",
    "predictions = [id_to_label[label.item()] for label in predictions]\n",
    "# predictions, actual_labels, all_words = predict_full_model(model, test_loader)\n",
    "char_accuracy = compute_character_accuracy(actuals, predictions)\n",
    "# word_boundaries = [batch[\"word_indices\"] for batch in test_loader]  # Assuming this is how word boundaries are stored.\n",
    "word_accuracy = compute_word_accuracy(actuals, predictions, word_lengths)\n",
    "\n",
    "print(f\"Character-level accuracy: {char_accuracy:.2%}\")\n",
    "print(f\"Word-level accuracy: {word_accuracy:.2%}\")\n",
    "# convert the labels to names\n",
    "actuals, predictions, label_names = convert_labels_to_names(actuals, predictions, label_to_id)\n",
    "plot_confusion_matrix(actuals, predictions, list(label_names.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the dual model class, it will be composed of two models.\n",
    "#### whenever there is a disagreement between the two models, the model will add nikud using the lookahead model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_sentence_predictions shape:  torch.Size([32, 100])\n",
      "directional_predictions shape:  torch.Size([32, 100])\n",
      "word_indices shape:  torch.Size([32, 100, 2])\n",
      "dual_model_predictions shape:  32\n"
     ]
    }
   ],
   "source": [
    "class DuelModel():\n",
    "    def __init__(self, full_sentence_model, directional_model):\n",
    "        self.full_sentence_model = full_sentence_model\n",
    "        self.directional_model = directional_model\n",
    "    \n",
    "    def predict_batch(self, batch):\n",
    "        self.full_sentence_model.eval()\n",
    "        self.directional_model.eval()\n",
    "        dual_model_predictions = []\n",
    "        dual_model_predictions_words = []\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            word_indices = batch['word_indices'].to(device)\n",
    "\n",
    "            full_sentence_logits = self.full_sentence_model.forward(input_ids)\n",
    "            directional_logits = self.directional_model.forward(input_ids, word_indices)\n",
    "\n",
    "            full_sentence_predictions = torch.argmax(full_sentence_logits, dim=2)\n",
    "            directional_predictions = torch.argmax(directional_logits, dim=2)\n",
    "            # iterate over each prediction\n",
    "            for i in range(len(full_sentence_predictions)):\n",
    "                current_prediction = []\n",
    "                current_prediction_words = []\n",
    "                # remove padding from the prediction\n",
    "                full_sentence_prediction = full_sentence_predictions[i][attention_mask[i] == 1]\n",
    "                directional_prediction = directional_predictions[i][attention_mask[i] == 1]\n",
    "\n",
    "                # for each token in the prediction, check if both models agree, if not - use the full sentence model prediction, otherwise change to 0\n",
    "                for j in range(len(full_sentence_prediction)):\n",
    "                    if full_sentence_prediction[j] != directional_prediction[j]:\n",
    "                        current_prediction.append(full_sentence_prediction[j].item())\n",
    "                    else:\n",
    "                        current_prediction.append(-1)\n",
    "                dual_model_predictions.append(current_prediction)\n",
    "\n",
    "                word_indices_of_current_prediction = word_indices[i][word_indices[i][:, 0] != -1]\n",
    "                word_indices_of_current_prediction = [word.tolist() for word in word_indices_of_current_prediction]\n",
    "                current_prediction_words = list(map(lambda x: x[1]-x[0]+1, word_indices_of_current_prediction))\n",
    "                dual_model_predictions_words.append(current_prediction_words)\n",
    "\n",
    "        return dual_model_predictions, dual_model_predictions_words\n",
    "            \n",
    "\n",
    "\n",
    "dual_model = DuelModel(full_sentence_model, reading_direction_model)\n",
    "# get the first batch of test_loader which doesn't change between runs\n",
    "batch = next(iter(test_loader))\n",
    "dual_model_predictions, dual_model_predictions_words = dual_model.predict_batch(batch)\n",
    "for i, prediction in enumerate(dual_model_predictions):\n",
    "    # print(f\"prediction {i}: {prediction}\")\n",
    "    prediction_length = len(prediction)\n",
    "    prediction_words_sum = sum(dual_model_predictions_words[i])\n",
    "    # print(f\"prediction length: {prediction_length} vs. prediction words sum: {prediction_words_sum}\")\n",
    "    if prediction_length != prediction_words_sum:\n",
    "        print(f\"error in prediction {i}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the dual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
